{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the target url and if you want to do a delta load\n",
    "from cloud_upload_config import *\n",
    "\n",
    "if as_string == 'all string':\n",
    "    as_string = True\n",
    "else:\n",
    "    as_string = False\n",
    "\n",
    "if delta == 'full':\n",
    "    delta = False\n",
    "else:\n",
    "    delta = True\n",
    "    \n",
    "if exclude_loaded != 'skip':\n",
    "    exclude_loaded = False\n",
    "else:\n",
    "    exclude_loaded = True\n",
    "\n",
    "look_for_sap_files_globally = False\n",
    "path_to_folder = ''\n",
    "continue_from_last_time = False\n",
    "# this determines how detailed the log is, where INFO is the standard. the list below is ordered from most detailed (DEBUG) to least detailled (CRITICAL)\n",
    "# logging.DEBUG\n",
    "# logging.INFO\n",
    "# logging.WARNING\n",
    "# logging.ERROR\n",
    "# logging.CRITICAL\n",
    "#log_level = logging.DEBUG\n",
    "\n",
    "\n",
    "\n",
    "global compressed\n",
    "compressed = ['.tar', '.gz', '.zip', '.7z']\n",
    "\n",
    "global generic_file_type\n",
    "generic_file_type = ['.csv', '.xlsx', '.xls', '.parquet']\n",
    "\n",
    "global sap_file_type\n",
    "sap_file_type = '(.*)_[0-9]{8}_[0-9]{6}.'\n",
    "\n",
    "global encrypted\n",
    "encrypted = ['.gpg', '.pgp']\n",
    "\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "#logname = f'IBC_Loader_log_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log'\n",
    "FORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
    "formatter = logging.Formatter(FORMAT)\n",
    "logging.basicConfig(format=FORMAT, filename=logname, level=logging.INFO)\n",
    "print(logname)\n",
    "logging.info('logging initialized')\n",
    "\n",
    "try:\n",
    "    import re\n",
    "    import subprocess\n",
    "    import json\n",
    "    import requests\n",
    "    import os\n",
    "    import py7zr\n",
    "    import tarfile\n",
    "    from io import BytesIO\n",
    "    import zipfile\n",
    "    from itertools import repeat\n",
    "    import gzip\n",
    "    import pandas as pd\n",
    "    from pandas.errors import EmptyDataError\n",
    "    import numpy as np\n",
    "    from multiprocessing import get_context\n",
    "    from pycelonis import get_celonis\n",
    "    from pycelonis.utils import parquet_utils\n",
    "    from chardet.universaldetector import UniversalDetector\n",
    "    import pycelonis\n",
    "    import fastparquet as fp\n",
    "    import pyarrow as pa\n",
    "    import time\n",
    "    import csv\n",
    "    from EMS_cloud_module import cloud\n",
    "    from EMS_classes import ibc_team, bucket, folder, ibc_file\n",
    "    import itertools\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    from pathlib import Path\n",
    "    from itertools import product\n",
    "    import copy\n",
    "    import sys\n",
    "    import traceback as tb\n",
    "    from mp_sap import sap_load\n",
    "    #from lib.cloud_module import cloud\n",
    "    #from lib.upload_module import import_sap_header\n",
    "except ModuleNotFoundError as e:\n",
    "    logging.error(e)\n",
    "    logging.error('please install missing packages to use this program.')\n",
    "    print('shutting down')\n",
    "    quit()\n",
    "\n",
    "if agreed != 'yes':\n",
    "    logging.error('you need to read and accept the terms listed in disclaimer.md')\n",
    "    quit()\n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '16'\n",
    "\n",
    "def determine_tables_loaded(ibc_team):\n",
    "    # Create new table with the subset we're interested in\n",
    "    data = None\n",
    "    celonis = get_celonis()\n",
    "    logging.info('checking for tables that have already been loaded.')\n",
    "    random_name = f'zzz___TEMP___{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "    \n",
    "    if ibc_team.connectionid is None:\n",
    "        create_table_from_query_statement = f'CREATE TABLE IF NOT EXISTS \"{random_name}\" AS (SELECT table_name FROM tables WHERE table_schema = \\'{ibc_team.poolid}\\');'\n",
    "    else:\n",
    "        create_table_from_query_statement = f'CREATE TABLE IF NOT EXISTS \"{random_name}\" AS (SELECT table_name FROM tables WHERE table_schema = \\'{ibc_team.poolid}_{ibc_team.connectionid}\\');'\n",
    "    # table_name FROM tables where table_schema = \\'\\'\n",
    "    \n",
    "    # Create data job and run table creation script\n",
    "    p = celonis.pools.find(ibc_team.poolid)\n",
    "    counter = 0\n",
    "    while counter < 4:\n",
    "        counter += 1\n",
    "        try:\n",
    "            dj = p.create_data_job(random_name)\n",
    "            transf = dj.create_transformation(random_name, create_table_from_query_statement)\n",
    "            transf.execute()\n",
    "\n",
    "            # Create temporary data model in pool and add recently created table, then reload\n",
    "            dm = p.create_datamodel(random_name)\n",
    "            try:\n",
    "                dm.add_table_from_pool(random_name)\n",
    "                dm.reload(from_cache=False, wait_for_reload=True)\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Find table object in data model and download\n",
    "                t = dm.tables.find(random_name)\n",
    "\n",
    "                path = t._get_data_file(Path('.') / random_name)\n",
    "                data = pd.read_parquet(path)\n",
    "            except Exception as e:\n",
    "                logging.error(f'determining what tables have been loaded failed with: {e}')\n",
    "            finally:\n",
    "                # Deleting temporary objects\n",
    "                dm.delete()\n",
    "                transf.statement = f'DROP TABLE IF EXISTS \"{random_name}\";'\n",
    "                transf.execute()\n",
    "                dj.delete()\n",
    "                if sys.version_info > (3,8):\n",
    "                    path.unlink(missing_ok=True)\n",
    "                else:\n",
    "                    path.unlink()\n",
    "            loaded_tables = pd.Series(data['table_name']).tolist()\n",
    "            try:\n",
    "                loaded_tables.remove(random_name)\n",
    "            except:\n",
    "                pass\n",
    "            logging.info(f'these tables are already in the data pool: {loaded_tables}')\n",
    "            break\n",
    "        except:\n",
    "            loaded_tables = []\n",
    "        logging.warning(f'determine_tables_loaded failed for {counter}. time. Retrying.')\n",
    "        time.sleep(1)\n",
    "    return loaded_tables\n",
    "\n",
    "def determine_line_count_of_loaded_tables(ibc_team):\n",
    "    # Create new table with the subset we're interested in\n",
    "    data = None\n",
    "    celonis = get_celonis()\n",
    "    logging.info('counting lines of tables that have been loaded.')\n",
    "    \n",
    "    random_name = f'zzz___TEMP_LC___{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "    add_line_counts_statement = []\n",
    "    add_line_counts_statement.append(f'CREATE TABLE IF NOT EXISTS \"{random_name}\" (\"TABLE\" VARCHAR(80), \"COUNT\" INTEGER);\\n')\n",
    "    \n",
    "    tables = determine_tables_loaded(ibc_team)\n",
    "    \n",
    "    \n",
    "    if ibc_team.connectionid is None:\n",
    "        for t in tables:\n",
    "            add_line_counts_statement.append(f'INSERT INTO \"{random_name}\" (\"TABLE\" ,\"COUNT\") SELECT \\'{t}\\', COUNT(1) FROM \"{t}\";\\n')\n",
    "    else:\n",
    "        for t in tables:\n",
    "            add_line_counts_statement.append(f'INSERT INTO \"{random_name}\" (\"TABLE\" ,\"COUNT\") SELECT \\'{t}\\', COUNT(1) FROM <%=DATASOURCE:JDBC%>.\"{t}\";\\n')\n",
    "    add_line_counts_statement = ''.join(add_line_counts_statement)\n",
    "    \n",
    "    # table_name FROM tables where table_schema = \\'\\'\n",
    "    # Create data job and run table creation script\n",
    "    p = celonis.pools.find(ibc_team.poolid)\n",
    "    counter = 0\n",
    "    while counter < 4:\n",
    "        counter += 1\n",
    "        try:\n",
    "            dj = p.create_data_job(random_name)\n",
    "            transf = dj.create_transformation(random_name, add_line_counts_statement)\n",
    "            transf.execute(wait_for_execution=True)\n",
    "            # Create temporary data model in pool and add recently created table, then reload\n",
    "            dm = p.create_datamodel(random_name)\n",
    "            try:\n",
    "                dm.add_tables_from_pool(random_name)\n",
    "                dm.reload(from_cache=False, wait_for_reload=True)\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Find table object in data model and download\n",
    "                t = dm.tables.find(random_name)\n",
    "\n",
    "                path = t._get_data_file(Path('.') / random_name)\n",
    "                data = pd.read_parquet(path)\n",
    "            except Exception as e:\n",
    "                logging.error(f'determining line count per table failed with: {e}')\n",
    "            finally:\n",
    "                # Deleting temporary objects\n",
    "                dm.delete()\n",
    "                transf.statement = f'DROP TABLE IF EXISTS \"{random_name}\";'\n",
    "                transf.execute()\n",
    "                dj.delete()\n",
    "                if sys.version_info > (3,8):\n",
    "                    path.unlink(missing_ok=True)\n",
    "                else:\n",
    "                    path.unlink()\n",
    "            #logging.info(f'these tables are already in the data pool: {loaded_tables}')\n",
    "            break\n",
    "        except:\n",
    "            loaded_tables = []\n",
    "        logging.warning(f'determine_tables_loaded failed for {counter}. time. Retrying.')\n",
    "        time.sleep(1)\n",
    "    return data\n",
    "    #data.to_excel('lines.xlsx')\n",
    "\n",
    "def remove_file_endings(filename):\n",
    "    endings = generic_file_type + compressed\n",
    "    endings.extend(list(map(lambda x: x.upper(), endings)))\n",
    "    for ending in endings:\n",
    "        filename = filename.replace(ending, '')\n",
    "    return filename\n",
    "\n",
    "def clean_table_name(name):\n",
    "    name = remove_file_endings(name)\n",
    "    name = name.replace('.filepart', '')\n",
    "    name = name.replace('.', '/')\n",
    "    return re.sub('[^A-Za-z0-9_/]', '_', name)\n",
    "\n",
    "    \n",
    "def decider(indicator, value):\n",
    "    if indicator == 'float':\n",
    "        return flt(value)\n",
    "    elif indicator == 'int':\n",
    "        return inti(value)\n",
    "    elif indicator == 'time':\n",
    "        return time_casting(value)\n",
    "    else:\n",
    "        return dt(value)\n",
    "\n",
    "def time_casting(df):\n",
    "    try:\n",
    "        df = '19700101'+df\n",
    "        return dt(df)\n",
    "    except Exception as e:\n",
    "        print(f'casting {df} to time failed with {e}')\n",
    "\n",
    "def dt(df):\n",
    "    try:\n",
    "        df = pd.to_datetime(df, errors='coerce')\n",
    "        return df.astype('datetime64')\n",
    "    except Exception as e:\n",
    "        print(f'casting {df} to datetime failed with {e}')\n",
    "\n",
    "def flt(df):\n",
    "    try:\n",
    "        mask = df.str.contains('-')\n",
    "        df = df.str.replace('-', '').str.strip()\n",
    "        df = pd.to_numeric(df, downcast='float', errors='coerce')\n",
    "        #df = df.astype(float)\n",
    "        return df.mask(mask, -df)\n",
    "    except Exception as e:\n",
    "        logging.error(f'casting {df} to float failed with {e}')\n",
    "\n",
    "def inti(df):\n",
    "    try:\n",
    "        mask = df.str.contains('-')\n",
    "        df = df.str.replace('-', '').str.strip()\n",
    "        df = pd.to_numeric(df, downcast='signed', errors='coerce')\n",
    "        #df = df.str.replace('-', '').str.strip().astype('int64')\n",
    "        #df = df.astype(int)\n",
    "        return df.mask(mask, -df)\n",
    "    except Exception as e:\n",
    "        logging.error(f'casting {df} to int failed with {e}')\n",
    "\n",
    "def type_determination(x):\n",
    "    if x in ['CURR', 'QUAN', 'DEC', 'FLTP']:\n",
    "        return 'float'\n",
    "    elif x in ['INT1', 'INT2', 'INT4', 'PREC']:\n",
    "        return 'int'\n",
    "    elif x in ['DATS']:\n",
    "        return 'date'\n",
    "    elif x in ['TIMS']:\n",
    "        return 'time'\n",
    "    else:\n",
    "        return 'str'\n",
    "\n",
    "    \n",
    "def import_sap_header(header, files, jobstatus, uppie, data, location_indicator='local', delta=False, pwd=None):\n",
    "    encoding = 'utf-8'\n",
    "    \"\"\"\n",
    "    team: {self.team}\n",
    "                    realm: {self.realm}\n",
    "                    poolid: {self.poolid}\n",
    "                    connectionid: {self.connectionid}\n",
    "                    appkey: {self.appkey}\n",
    "                    apikey: {self.apikey}\n",
    "                    url: {self.url}\n",
    "    \"\"\"\n",
    "    logging.debug(\"header file: %s passed to import_sap_header\", header.file)\n",
    "    if location_indicator == 'local':\n",
    "        ref = header.file.split('HEADER')\n",
    "        ref[1] = ref[1].replace('.csv', '')\n",
    "    elif location_indicator == 'global':\n",
    "        ref = Path(header.file).name.split('HEADER')\n",
    "        ref[1] = ref[1].replace('.csv', '')\n",
    "    else:\n",
    "        logging.error(f'location indicator {location_indicator} is invalid')\n",
    "        raise ValueError(f'location indicator {location_indicator} is invalid')\n",
    "    targetname = Path(ref[0][:-1]).name #.replace(path_to_file, '')\n",
    "    targetname = clean_table_name(targetname)\n",
    "    if targetname in data:\n",
    "        logging.warning(f'skipping {header.file} as table with {targetname} is already present in target pool.')\n",
    "    else:\n",
    "        jobhandle = uppie.create_job(pool_id=header.poolid,\n",
    "                             data_connection_id=header.connectionid,\n",
    "                             targetName=targetname,\n",
    "                             upsert=delta)\n",
    "        logging.info(f'starting to upload {targetname}')\n",
    "        logging.debug(jobhandle)\n",
    "        jobstatus[jobhandle['id']] = False\n",
    "        url = f'https://{header.team}.{header.realm}.celonis.cloud/storage-manager/api/buckets/{header.bucket_id}/files?path=/' + header.file\n",
    "        header_json = {'Authorization': 'AppKey {}'.format(header.appkey), 'Accept': 'application/octet-stream'}\n",
    "        with requests.get(url, headers=header_json, stream=False) as r:\n",
    "            r.raise_for_status()\n",
    "            df = pd.read_csv(BytesIO(r.content), header=None, dtype=str, sep=' ', names=['names', 'type', 'length', 'declength'], encoding=encoding)\n",
    "\n",
    "        df['type'] = df['type'].apply(lambda x: type_determination(x))\n",
    "        type_dict = {}\n",
    "        if len(df[df['type'] == 'float']) > 0:\n",
    "            type_dict['float'] = list(df[df['type'] == 'float']['names'])\n",
    "        if len(df[df['type'] == 'int']) > 0:\n",
    "            type_dict['int'] = list(df[df['type'] == 'int']['names'])\n",
    "        if len(df[df['type'] == 'date']) > 0:\n",
    "            type_dict['date'] = list(df[df['type'] == 'date']['names'])\n",
    "        if len(df[df['type'] == 'time']) > 0:\n",
    "            type_dict['time'] = list(df[df['type'] == 'time']['names'])\n",
    "        relevant_files = (f for f in files if ref[0] in f.file and ref[1] in f.file)\n",
    "        #logging.info(f'{ref[0]}, {ref[1]}, {relevant_files}')\n",
    "\n",
    "        header_json = {'Authorization': 'AppKey {}'.format(header.appkey), 'Accept': 'application/octet-stream'}\n",
    "        pre_url = f'https://{header.team}.{header.realm}.celonis.cloud/storage-manager/api/buckets/{header.bucket_id}/files?path=/'\n",
    "        with get_context(\"forkserver\").Pool(5) as pool:\n",
    "            logging.info('jumping into the pool')\n",
    "            parallel_jobs = pool.imap_unordered(sap_load, zip(relevant_files, repeat(pre_url), repeat(header_json), repeat(header), repeat(jobhandle['id']), repeat(df), repeat(type_dict), repeat(uppie)))\n",
    "            for parallel_job in parallel_jobs:\n",
    "                pass\n",
    "        logging.info('upload done, submitting job...')\n",
    "        uppie.submit_job(pool_id=header.poolid, job_id=jobhandle['id'])\n",
    "        data.append(targetname)\n",
    "    return data\n",
    "\n",
    "\n",
    "def test_float(strg: str) -> bool:\n",
    "    try:\n",
    "        float(strg)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def detect_encoding(non_sap_file, pwd):\n",
    "    logging.info(f'starting encoding determination')\n",
    "    detector = UniversalDetector()\n",
    "    detector.reset()\n",
    "    #counter = 0\n",
    "    try:\n",
    "        \"\"\"\n",
    "        with open(file, 'rb') as file_detect:\n",
    "        \"\"\"\n",
    "        url = f'https://{non_sap_file.team}.{non_sap_file.realm}.celonis.cloud/storage-manager/api/buckets/{non_sap_file.bucket_id}/files?path=/' + non_sap_file.file\n",
    "        header_json = {'Authorization': 'AppKey {}'.format(non_sap_file.appkey), 'Accept': 'application/octet-stream'}\n",
    "        with requests.get(url, headers=header_json, stream=True) as file_detect:\n",
    "            file_detect.raise_for_status()\n",
    "            if non_sap_file.file.split('.')[-1] == 'zip':\n",
    "                z = zipfile.ZipFile(BytesIO(file_detect.content))\n",
    "                zip_content = z.infolist()\n",
    "                idx = 0\n",
    "                for c, zips in enumerate(zip_content):\n",
    "                    if '.csv' in zips.filename:\n",
    "                        idx = c\n",
    "                        break\n",
    "                logging.debug(zip_content[idx])\n",
    "                fh = z.open(zip_content[idx])\n",
    "            elif non_sap_file.file.split('.')[-1] == '7z':\n",
    "                z = py7zr.SevenZipFile(BytesIO(file_detect.content), password=pwd)\n",
    "                filename = z.getnames()[0]\n",
    "                fh = z.read(filename)[filename]\n",
    "                logging.debug(fh)\n",
    "            elif non_sap_file.file.split('.')[-2] == 'tar':\n",
    "                fh = tarfile.open(fileobj=BytesIO(file_detect.content), mode='r:gz')\n",
    "            elif non_sap_file.file.split('.')[-1] == 'gz':\n",
    "                fh = gzip.GzipFile(fileobj=BytesIO(file_detect.content), mode='rb')\n",
    "            else:\n",
    "                fh = BytesIO(file_detect.content)\n",
    "            for counter, line in enumerate(fh):\n",
    "                #counter += 1\n",
    "                detector.feed(line)\n",
    "                if detector.done:\n",
    "                    break\n",
    "                elif counter > 50000:\n",
    "                    break\n",
    "        detector.close()\n",
    "        enc = detector.result['encoding'].lower()\n",
    "        logging.info(f'{non_sap_file.file} has encoding: {detector.result}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'encoding detection failed with: {e}\\nreverting to utf-8 as standard')\n",
    "        enc = 'utf-8'\n",
    "    finally:\n",
    "        detector.reset()\n",
    "    return enc\n",
    "\n",
    "def determine_dialect(non_sap_file, enc):\n",
    "    logging.info('starting dialect determination')\n",
    "    sniffer = csv.Sniffer()\n",
    "    sniffer.preferred = [';', ',', '\\t', '|', '~', ' ']\n",
    "    dialect = ''\n",
    "    data = []\n",
    "    counter = 0\n",
    "    try:\n",
    "        url = f'https://{non_sap_file.team}.{non_sap_file.realm}.celonis.cloud/storage-manager/api/buckets/{non_sap_file.bucket_id}/files?path=/' + non_sap_file.file\n",
    "        header_json = {'Authorization': 'AppKey {}'.format(non_sap_file.appkey), 'Accept': 'application/octet-stream'}\n",
    "        with requests.get(url, headers=header_json, stream=True) as file_detect:\n",
    "            file_detect.raise_for_status()\n",
    "            if non_sap_file.file.split('.')[-1] == 'zip':\n",
    "                z = zipfile.ZipFile(BytesIO(file_detect.content))\n",
    "                zip_content = z.infolist()\n",
    "                idx = 0\n",
    "                for c, zips in enumerate(zip_content):\n",
    "                    if '.csv' in zips.filename:\n",
    "                        idx = c\n",
    "                        break\n",
    "                logging.debug(zip_content[idx])\n",
    "                fh = z.open(zip_content[idx])\n",
    "            elif non_sap_file.file.split('.')[-1] == '7z':\n",
    "                z = py7zr.SevenZipFile(BytesIO(file_detect.content), password=pwd)\n",
    "                filename = z.getnames()[0]\n",
    "                fh = z.read(filename)[filename]\n",
    "                logging.debug(fh)\n",
    "            elif non_sap_file.file.split('.')[-2] == 'tar':\n",
    "                fh = tarfile.open(fileobj=BytesIO(file_detect.content), mode='r:gz')\n",
    "            elif non_sap_file.file.split('.')[-1] == 'gz':\n",
    "                fh = gzip.GzipFile(fileobj=BytesIO(file_detect.content), mode='rb')\n",
    "            else:\n",
    "                fh = BytesIO(file_detect.content)\n",
    "            for counter, line in enumerate(fh):\n",
    "                data.append(line.decode(enc))\n",
    "                counter += 1\n",
    "                if counter == 10:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "    try:\n",
    "        data_str = ''.join(data)\n",
    "        dialect = sniffer.sniff(data_str)\n",
    "        delimiter = dialect.delimiter\n",
    "        quotechar = dialect.quotechar\n",
    "        escapechar = dialect.escapechar\n",
    "        header = sniffer.has_header(data_str)\n",
    "    except:\n",
    "        logging.warning('''sniffer was unsuccessful, using a simplistic approach\n",
    "                        to determine the delimiter and existence of header.''')\n",
    "        line1 = list_get(data, 0)\n",
    "        delim = dict()\n",
    "        for i in [';', ',', '\\t', '|']:\n",
    "            delim[i] = len(line1.split(i))\n",
    "        delimiter = sorted(delim.items(), key=lambda kv: kv[1])[-1][0]\n",
    "        quotechar = None\n",
    "        escapechar = None\n",
    "        if any(map(test_float, line1.split(delimiter))):\n",
    "            header = False\n",
    "        else:\n",
    "            header = True\n",
    "    if header is True:\n",
    "        header = 0\n",
    "    else:\n",
    "        header = None\n",
    "    logging.info(f'''delimiter: {delimiter}, quotechar: {quotechar},\n",
    "                     escapechar: {escapechar}, header: {header}''')\n",
    "    return {'delimiter': delimiter, 'quotechar': quotechar,\n",
    "            'escapechar': escapechar, 'header': header}\n",
    "\n",
    "def list_to_str(lst):\n",
    "    return_list = list()\n",
    "    for i in lst:\n",
    "        return_list.append(str(i))\n",
    "    return return_list\n",
    "\n",
    "def list_get(lst, index):\n",
    "    if len(lst) < index +1:\n",
    "        return ''\n",
    "    else:\n",
    "        return lst[index]\n",
    "\n",
    "def delete_file_from_sftp(file):\n",
    "    url = f'https://{file.team}.{file.realm}.celonis.cloud/storage-manager/api/buckets/{file.bucket_id}/files?path=/' + file.file\n",
    "    header_json = {'Authorization': f'AppKey {file.appkey}', 'Accept': 'application/json'}\n",
    "    r = requests.delete(url, headers=header_json)\n",
    "    logging.warning(f'deletion status: {r.status_code}')\n",
    "\n",
    "def import_non_sap_file(non_sap_file,\n",
    "                        jobstatus,\n",
    "                        uppie,\n",
    "                        data,\n",
    "                        delta,\n",
    "                        as_string,\n",
    "                        pwd=None,\n",
    "                        encoding_list=[None, 'utf-8', 'ascii', 'cp1252', 'latin_1', 'iso-8859-1'],\n",
    "                        no_quoting=False,\n",
    "                        ):\n",
    "    encoding = 'utf-8'\n",
    "    \"\"\"\n",
    "    team: {self.team}\n",
    "                    realm: {self.realm}\n",
    "                    poolid: {self.poolid}\n",
    "                    connectionid: {self.connectionid}\n",
    "                    appkey: {self.appkey}\n",
    "                    apikey: {self.apikey}\n",
    "                    url: {self.url}\n",
    "\n",
    "    if non_sap_file.size > 350 * 1024 * 1024:\n",
    "        logging.warning(f'skipped file {non_sap_file.file} because it was too large.')\n",
    "        return None#\n",
    "    \"\"\"\n",
    "    targetname = Path(non_sap_file.file).name\n",
    "    targetname = clean_table_name(targetname)\n",
    "    if targetname in data:\n",
    "        logging.warning(f'skipping {non_sap_file.file} as table with {targetname} is already present in target pool.')\n",
    "    else:\n",
    "        jobhandle = uppie.create_job(pool_id=non_sap_file.poolid,\n",
    "                             data_connection_id=non_sap_file.connectionid,\n",
    "                             targetName=targetname,\n",
    "                             upsert=delta)\n",
    "        logging.debug(jobhandle)\n",
    "        try:\n",
    "            jobstatus[jobhandle['id']] = False\n",
    "        except KeyError:\n",
    "            logging.error(f'failed to get job id for file {non_sap_file} with job: {jobhandle}')\n",
    "            return None\n",
    "        logging.info(f'starting to upload {targetname}')\n",
    "        try:\n",
    "            url = f'https://{non_sap_file.team}.{non_sap_file.realm}.celonis.cloud/storage-manager/api/buckets/{non_sap_file.bucket_id}/files?path=/' + non_sap_file.file\n",
    "            header_json = {'Authorization': 'AppKey {}'.format(non_sap_file.appkey), 'Accept': 'application/octet-stream'}\n",
    "            with requests.get(url, headers=header_json, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                if non_sap_file.file.split('.')[-1] == 'zip':\n",
    "                    z = zipfile.ZipFile(BytesIO(r.content))\n",
    "                    zip_content = z.infolist()\n",
    "                    idx = 0\n",
    "                    for c, zips in enumerate(zip_content):\n",
    "                        if '.csv' in zips.filename:\n",
    "                            idx = c\n",
    "                            break\n",
    "                    logging.debug(zip_content[idx])\n",
    "                    fh = z.open(zip_content[idx])\n",
    "                elif non_sap_file.file.split('.')[-1] == '7z':\n",
    "                    z = py7zr.SevenZipFile(BytesIO(r.content), password=pwd)\n",
    "                    filename = z.getnames()[0]\n",
    "                    fh = z.read(filename)[filename]\n",
    "                    logging.debug(fh)\n",
    "                elif non_sap_file.file.split('.')[-2] == 'tar':\n",
    "                    fh = tarfile.open(fileobj=BytesIO(r.content), mode='r:gz')\n",
    "                elif non_sap_file.file.split('.')[-1] == 'gz':\n",
    "                    fh = gzip.GzipFile(fileobj=BytesIO(r.content), mode='rb')\n",
    "                else:\n",
    "                    fh = BytesIO(r.content)\n",
    "                if non_sap_file.file_type == '.parquet':\n",
    "                    tmp_parquet_file = Path('/home/jovyan/tmp.parquet')\n",
    "                    with open(tmp_parquet_file, 'wb') as out:\n",
    "                        out.write(fh.getvalue())\n",
    "                    df = parquet_utils.read_parquet(tmp_parquet_file)\n",
    "                    tmp_parquet_file.unlink()\n",
    "                    uppie.push_new_chunk(pool_id=non_sap_file.poolid,\n",
    "                                         job_id=jobhandle['id'],\n",
    "                                         dataframe=df,\n",
    "                                         )\n",
    "                elif non_sap_file.file_type == '.csv':\n",
    "                    if encoding_list[0] is None:\n",
    "                        encoding = detect_encoding(non_sap_file, pwd)\n",
    "                        if encoding == \"ascii\":\n",
    "                            encoding = \"utf-8\"\n",
    "                    else:\n",
    "                        encoding = encoding_list[0]\n",
    "                    dialect = determine_dialect(non_sap_file, encoding)\n",
    "                    delimiter = dialect['delimiter']\n",
    "                    quotechar = dialect['quotechar']\n",
    "                    escapechar = dialect['escapechar']\n",
    "                    if escapechar is None:\n",
    "                        escapechar = '\\\\'\n",
    "                    header = dialect['header']\n",
    "                    pd_config = {\n",
    "                                'filepath_or_buffer': fh,\n",
    "                                'encoding': encoding,\n",
    "                                'sep': delimiter,\n",
    "                                'parse_dates': False,\n",
    "                                'on_bad_lines': 'warn',\n",
    "                                'escapechar': escapechar,\n",
    "                                # TODO: Change chunksize to be dependend on # of cols (info can be taken from dialect determination)\n",
    "                                'chunksize': 200000,\n",
    "                                'engine': 'python',\n",
    "                                'keep_default_na': False,\n",
    "                                'header': header\n",
    "                                }\n",
    "                    if as_string is True:\n",
    "                        pd_config['dtype'] = str\n",
    "                    if no_quoting is True:\n",
    "                        pd_config['quoting'] = 3\n",
    "                    df_up = pd.read_csv(**pd_config)\n",
    "                    for i in df_up:\n",
    "                        logging.debug(i.head())\n",
    "                        i.columns = list_to_str(list(i.columns))\n",
    "                        uppie.push_new_chunk(pool_id=non_sap_file.poolid,\n",
    "                                             job_id=jobhandle['id'],\n",
    "                                             dataframe=i,\n",
    "                                             )\n",
    "                else:\n",
    "                    matches = {}\n",
    "                    pd_config = {\n",
    "                                'io': fh,\n",
    "                                'sheet_name': None,\n",
    "                                'keep_default_na': False,\n",
    "                                }\n",
    "                    if as_string is True:\n",
    "                        pd_config['dtype'] = str\n",
    "                    df = pd.read_excel(**pd_config)\n",
    "                    for a, b in product(df, df):\n",
    "                        col_a = df[a].columns\n",
    "                        col_b = df[b].columns\n",
    "                        if (len(col_a) == len(col_b)\n",
    "                            and (len([i for i, j in zip(col_a, col_b) if i == j])\n",
    "                                 == len(col_b))):\n",
    "                            matches[str(col_b)] = (matches.get(str(col_b), [a, b])\n",
    "                                                   + [a, b])\n",
    "                    for i in matches:\n",
    "                        matches[i] = set(matches[i])\n",
    "                    if (len(matches) == 1\n",
    "                        and len(df) == 1\n",
    "                        and len(copy.deepcopy(matches).popitem()[1]) == len(df)):\n",
    "                        for i in df:\n",
    "                            df = df[i]\n",
    "                    elif (len(matches) == 1\n",
    "                          and len(copy.deepcopy(matches).popitem()[1]) == len(df)):\n",
    "                        dfs = []\n",
    "                        for i in df:\n",
    "                            dfs.append(df[i])\n",
    "                        df = pd.concat(dfs, ignore_index=True)\n",
    "                    logging.debug(df.head())\n",
    "                    uppie.push_new_chunk(pool_id=non_sap_file.poolid,\n",
    "                                         job_id=jobhandle['id'],\n",
    "                                         dataframe=df,\n",
    "                                         )\n",
    "        except EmptyDataError:\n",
    "            logging.warning(f'{non_sap_file} is empty and will be skipped')\n",
    "            del jobstatus[jobhandle['id']]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            # TODO: add submission of job to exception as well as job deletion and removal from dict\n",
    "            logging.error(f'{non_sap_file} failed with error: {e}')\n",
    "            logging.exception(''.join(tb.format_exception(None, e, e.__traceback__)))\n",
    "            time.sleep(2)\n",
    "            if f\"'{delimiter}' expected after\" in str(e) and no_quoting is False:\n",
    "                del jobstatus[jobhandle['id']]\n",
    "                logging.info(f'retrying {non_sap_file} without quoting')\n",
    "                import_non_sap_file(non_sap_file,\n",
    "                                        jobstatus,\n",
    "                                        uppie,\n",
    "                                        data,\n",
    "                                        pwd,\n",
    "                                        delta,\n",
    "                                        as_string,\n",
    "                                        encoding_list,\n",
    "                                        no_quoting=True,\n",
    "                                        )\n",
    "            elif len(encoding_list) > 0:\n",
    "                encoding_list.pop(0)\n",
    "                if len(encoding_list) > 0:\n",
    "                    del jobstatus[jobhandle['id']]\n",
    "                    import_non_sap_file(non_sap_file,\n",
    "                                            jobstatus,\n",
    "                                            uppie,\n",
    "                                            data,\n",
    "                                            pwd,\n",
    "                                            delta,\n",
    "                                            as_string,\n",
    "                                            encoding_list,\n",
    "                                            )\n",
    "        uppie.submit_job(pool_id=non_sap_file.poolid, job_id=jobhandle['id'])\n",
    "        logging.info(f'finished uploading {non_sap_file.file}')\n",
    "        data.append(targetname)\n",
    "    return data\n",
    "\n",
    "def ibc_files_to_json(lst, name):\n",
    "    temp = []\n",
    "    for i in lst:\n",
    "        temp.append(i.to_dict())\n",
    "    with open(name, 'w') as out:\n",
    "        out.write(json.dumps(temp, indent=4))\n",
    "\n",
    "def json_to_ibc_files(jsn, url):\n",
    "    with open(jsn, 'r') as inp:\n",
    "        text = inp.read()\n",
    "    dct_lst = json.loads(text)\n",
    "    ibc_files = []\n",
    "    for entry in dct_lst:\n",
    "        entry_dct = entry\n",
    "        entry_dct['url'] = url\n",
    "        ibc_files.append(ibc_file(**entry_dct))\n",
    "    return ibc_files\n",
    "\n",
    "def check_permissions(c: pycelonis.celonis_api.celonis.Celonis):\n",
    "    missing = list()\n",
    "    for p in c.permissions:\n",
    "        if p[\"serviceName\"] == \"storage-manager\":\n",
    "            for perm in [\"GET\", \"LIST\"]:\n",
    "                if perm not in p[\"permissions\"]:\n",
    "                    missing.append(f\"File Storage Manager is missing {perm}\")\n",
    "        elif p[\"serviceName\"] == \"event-collection\":\n",
    "            for perm in [\"EDIT_ALL_DATA_POOLS\"]:\n",
    "                if perm not in p[\"permissions\"]:\n",
    "                    missing.append(f\"Data Integration is missing {perm}\")\n",
    "        elif p[\"serviceName\"] == \"ml-workbench\":\n",
    "            for perm in [\"EDIT_SCHEDULERS\", \"USE_ALL_SCHEDULERS\", \"CREATE_SCHEDULERS\", \"VIEW_CONFIGURATION\"]:\n",
    "                if perm not in p[\"permissions\"]:\n",
    "                    missing.append(f\"Machine Learning is missing {perm}\")\n",
    "    if len(missing) > 0:\n",
    "        e = \", \".join(missing)\n",
    "        raise PermissionError(1, \"The following permissions are missing\", e)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        check_permissions(get_celonis(key_type=\"APP_KEY\"))\n",
    "    except Exception as e:\n",
    "        logging.error(\"Permissions check failed with: %s\", e)\n",
    "        raise e\n",
    "\n",
    "    c = ibc_team(url)\n",
    "\n",
    "    if exclude_loaded is True:\n",
    "        data = determine_tables_loaded(c)\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    if continue_from_last_time is True and Path('./head.json').is_file():\n",
    "        logging.info('getting ibc_files from ML Workbench')\n",
    "        if Path('./body.json').is_file():\n",
    "            body = json_to_ibc_files('body.json', url)\n",
    "        else:\n",
    "            body = []\n",
    "        head = json_to_ibc_files('head.json', url)\n",
    "    else:\n",
    "        logging.info('getting ibc_files from SFTP')\n",
    "        head, body = [], []\n",
    "        buckets = c.find_buckets()\n",
    "        for b in buckets:\n",
    "            logging.info(f'started finding folders at: {datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "            f = b.find_folders(path_to_folder)\n",
    "            logging.info(f'finished finding folders at: {datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "            try:\n",
    "                logging.info(f'started classifying files at: {datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "                for i in f:\n",
    "                    head_instance, body_instance = i.classify_files()\n",
    "                    head.extend(head_instance)\n",
    "                    body.extend(body_instance)\n",
    "                logging.info(f'finished classifying files at: {datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "            except Exception as e:\n",
    "                logging.error(f'encounterd error {e} while processing {i} in {f}')\n",
    "            ibc_files_to_json(head, 'head.json')\n",
    "            ibc_files_to_json(body, 'body.json')\n",
    "\n",
    "    logging.info(f'finished classifying files. {len(head)} header and {len(body)} sub files were found.') # {lenght_before - len(head)} files were skipped.')\n",
    "\n",
    "    if look_for_sap_files_globally is True:\n",
    "        location_indicator = 'global'\n",
    "    else:\n",
    "        location_indicator = 'local'\n",
    "    jobstatus = {}\n",
    "    uppie = cloud(tenant=c.team, realm=c.realm, api_key=c.apikey)\n",
    "    for header in head:\n",
    "        if header.file_type == 'sap':\n",
    "            data = import_sap_header(header=header,\n",
    "                                     files=body,\n",
    "                                     jobstatus=jobstatus,\n",
    "                                     uppie=uppie,\n",
    "                                     data=data,\n",
    "                                     location_indicator=location_indicator,\n",
    "                                     delta=delta,\n",
    "                                     )\n",
    "        else:\n",
    "            data = import_non_sap_file(header, jobstatus, uppie, data, as_string, delta)\n",
    "\n",
    "    logging.info('upload done.')\n",
    "    error_flag = False\n",
    "    failed_tables = []\n",
    "    running = True\n",
    "    while running:\n",
    "        jobs = uppie.list_jobs(c.poolid)\n",
    "        for jobids in jobstatus:\n",
    "            for i in jobs:\n",
    "                try:\n",
    "                    if i['id'] == jobids:\n",
    "                        if i['status'] == 'QUEUED':\n",
    "                            pass\n",
    "                        elif jobstatus[jobids] is True:\n",
    "                            pass\n",
    "                        elif i['status'] == 'DONE':\n",
    "                            jobstatus[jobids] = True\n",
    "                        elif i['status'] != 'RUNNING':\n",
    "                            jobstatus[jobids] = True\n",
    "                        else:\n",
    "                            pass\n",
    "                        break\n",
    "                except (KeyboardInterrupt, SystemExit):\n",
    "                    logging.error('terminating program\\n')\n",
    "                    quit()\n",
    "                except:\n",
    "                    pass\n",
    "        if all(status is True for status in jobstatus.values()):\n",
    "            running = False\n",
    "            for i in jobs:\n",
    "                if i['id'] in jobstatus:\n",
    "                    if i['status'] == 'DONE':\n",
    "                        logging.info(f\"{i['targetName']} was successfully installed in the database\")\n",
    "                    else:\n",
    "                        error_flag = True\n",
    "                        failed_tables.append(i['targetName'])\n",
    "                        logging.error(f\"{i['targetName']} failed with: {i}\")\n",
    "        else:\n",
    "            time.sleep(15)\n",
    "    if error_flag is True:\n",
    "        raise ValueError(f'the loading of the following tables failed: {failed_tables}')\n",
    "    logging.info('all done.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}