{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the target url and if you want to do a delta load\n",
    "url = ''\n",
    "delta = False\n",
    "\n",
    "\n",
    "# this determines how detailed the log is, where INFO is the standard. the list below is ordered from most detailed (DEBUG) to least detailled (CRITICAL)\n",
    "# logging.DEBUG\n",
    "# logging.INFO\n",
    "# logging.WARNING\n",
    "# logging.ERROR\n",
    "# logging.CRITICAL\n",
    "log_level = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "\n",
    "logname = ''.join([datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "                   '_uploader.log'])\n",
    "FORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
    "formatter = logging.Formatter(FORMAT)\n",
    "logging.basicConfig(format=FORMAT, filename=logname, level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    import re\n",
    "    import subprocess\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    import zipfile\n",
    "    import pandas as pd\n",
    "    from pycelonis import get_celonis\n",
    "    import fastparquet as fp\n",
    "    import pyarrow as pa\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "except ModuleNotFoundError as e:\n",
    "    logging.error(e)\n",
    "    logging.error('please install missing packages to use this program.')\n",
    "    print('shutting down')\n",
    "    quit()\n",
    "\n",
    "class cloud:\n",
    "\n",
    "    def get_api(self, path):\n",
    "        return f\"https://{self.tenant}.{self.realm}.celonis.cloud/{path}\"\n",
    "\n",
    "    def __init__(self, tenant, realm, api_key):\n",
    "        self.tenant = tenant\n",
    "        self.realm = realm\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def get_jobs_api(self, pool_id):\n",
    "        return self.get_api(f\"integration/api/v1/data-push/{pool_id}/jobs/\")\n",
    "\n",
    "    def get_auth(self):\n",
    "        return {'authorization': f\"AppKey {self.api_key}\"}\n",
    "\n",
    "    def list_jobs(self, pool_id):\n",
    "        api = self.get_jobs_api(pool_id)\n",
    "        return requests.get(api, headers=self.get_auth()).json()\n",
    "\n",
    "    def delete_job(self, pool_id, job_id):\n",
    "        api = self.get_jobs_api(pool_id) + f\"/{job_id}\"\n",
    "        return requests.delete(api, headers=self.get_auth())\n",
    "\n",
    "    def create_job(self, pool_id, targetName, data_connection_id,\n",
    "                   upsert=False):\n",
    "        api = self.get_jobs_api(pool_id)\n",
    "        job_type = \"REPLACE\"\n",
    "        if upsert:\n",
    "            job_type = \"DELTA\"\n",
    "        if not data_connection_id:\n",
    "            payload = {'targetName': targetName, 'type': job_type,\n",
    "                       'dataPoolId': pool_id}\n",
    "        else:\n",
    "            payload = {'targetName': targetName, 'type': job_type,\n",
    "                       'dataPoolId': pool_id,\n",
    "                       'connectionId': data_connection_id}\n",
    "        r = requests.post(api, headers=self.get_auth(), json=payload)\n",
    "        return r.json()\n",
    "\n",
    "    def push_new_dir(self, pool_id, job_id, dir_path):\n",
    "        files = [join(dir_path, f) for f in listdir(dir_path)\n",
    "                 if isfile(join(dir_path, f))]\n",
    "        parquet_files = list(filter(lambda f: f.endswith(\".parquet\"), files))\n",
    "        for parquet_file in parquet_files:\n",
    "            logging.debug(f\"Uploading chunk {parquet_file}\")\n",
    "            self.push_new_chunk(pool_id, job_id, parquet_file)\n",
    "\n",
    "    def push_new_chunk(self, pool_id, job_id, file_path):\n",
    "        api = self.get_jobs_api(pool_id) + f\"/{job_id}/chunks/upserted\"\n",
    "        upload_file = {\"file\": file_path}\n",
    "        return requests.post(api, files=upload_file, headers=self.get_auth())\n",
    "\n",
    "    def submit_job(self, pool_id, job_id):\n",
    "        api = self.get_jobs_api(pool_id) + \"/{job_id}/\"\n",
    "        return requests.post(api, headers=self.get_auth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = []\n",
    "connectionflag = 1\n",
    "try:\n",
    "    parts.append(re.search('https://([a-z0-9-]+)\\.', url).groups()[0])\n",
    "    parts.append(re.search('\\.([a-z0-9-]+)\\.celonis', url).groups()[0])\n",
    "    parts.append(re.search('ui/pools/([a-z0-9-]+)', url).groups()[0])\n",
    "    try:\n",
    "        parts.append(re.search('data-connections/[a-z-]+/([a-z0-9-]+)', url)\n",
    "                     .groups()[0])\n",
    "    except AttributeError:\n",
    "        connectionflag = 0\n",
    "except AttributeError:\n",
    "    logging.error(f'{url} this is an unvalid url.')\n",
    "logging.info(connectionflag)\n",
    "\n",
    "team = parts[0]\n",
    "realm = parts[1]\n",
    "poolid = parts[2]\n",
    "\n",
    "if connectionflag == 1:\n",
    "    connectionid = parts[3]\n",
    "else:\n",
    "    connectionid = None\n",
    "\n",
    "\n",
    "cmd = 'printenv | grep CELONIS_API_TOKEN'\n",
    "appkey = subprocess.run(cmd, shell=True, capture_output=True)\n",
    "appkey = appkey.stdout.decode('utf-8').split('=')[1].strip()\n",
    "\n",
    "apikey = appkey\n",
    "\n",
    "\n",
    "url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets?feature=SFTP'\n",
    "header_json = {'Authorization': f'AppKey {appkey}', 'Accept': 'application/json'}\n",
    "file_response = requests.get(url, headers=header_json)\n",
    "buckets = [i['id'] for i in file_response.json()]\n",
    "logging.info('buckets found:')\n",
    "for bucket_id in buckets:\n",
    "    logging.info(bucket_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set bucket_id if you leave this blank a random id will be chosen\n",
    "bucket_id = ''\n",
    "\n",
    "if bucket_id == '':\n",
    "    for bucketed_id in buckets:\n",
    "        bucket_id = bucketed_id\n",
    "logging.info(f'bucket chosen: {bucket_id}')\n",
    "\n",
    "# set folder to scan (default is the root folder)\n",
    "path_to_file = ''\n",
    "encoding = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the file desciptions\n",
    "url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets?feature=SFTP'\n",
    "header_json = {'Authorization': f'AppKey {appkey}', 'Accept': 'application/json'}\n",
    "file_response = requests.get(url, headers=header_json)\n",
    "# get all the file names\n",
    "\n",
    "url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets/{bucket_id}/files?path=/' + path_to_file\n",
    "header_json = {'Authorization': f'AppKey {appkey}', 'Accept': 'application/json'}\n",
    "file_response = requests.get(url, headers=header_json)\n",
    "files = []\n",
    "folders = []\n",
    "try:\n",
    "    logging.info(file_response.json())\n",
    "    for i in file_response.json()['children']:\n",
    "        if i['type'] == 'FILE':\n",
    "            files.append(path_to_file + i['filename'])\n",
    "        elif i['type'] == 'DIRECTORY':\n",
    "            folders.append(i['filename'])\n",
    "except:\n",
    "    logging.error(file_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with all the headers in it that are most recent\n",
    "header = sorted([header for header in files if ('_HEADER' in header)], reverse=True)\n",
    "header2 = [header[0]]\n",
    "for i in header:\n",
    "    if all(i.split('HEADER')[0] not in h for h in header2):\n",
    "        header2.append(i)\n",
    "header = header2\n",
    "logging.info(header)\n",
    "jobstatus = {}\n",
    "uppie = cloud(tenant=team, realm=realm, api_key=apikey)\n",
    "\n",
    "# start the upload per header\n",
    "for i in header:\n",
    "    if True:\n",
    "        table_files = []\n",
    "        indices = []\n",
    "        ref = i.split('HEADER')\n",
    "        ref[1] = ref[1].replace('.csv', '')\n",
    "        targetname = ref[0][:-1].replace(path_to_file, '')\n",
    "        jobhandle = uppie.create_job(pool_id=poolid,\n",
    "                             data_connection_id=connectionid,\n",
    "                             targetName=targetname,\n",
    "                             upsert=delta)\n",
    "        logging.debug(jobhandle)\n",
    "        jobstatus[jobhandle['id']] = False\n",
    "        for n in range(len(files)):\n",
    "            if ref[0] in files[n] and ref[1] in files[n]:\n",
    "                indices.append(n)\n",
    "        for m in indices[::-1]:\n",
    "            table_files.append(files.pop(m))\n",
    "        url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets/{bucket_id}/files?path=/' + i\n",
    "        header_json = {'Authorization': 'AppKey {}'.format(appkey), 'Accept': 'application/octet-stream'}\n",
    "        with requests.get(url, headers=header_json, stream=False) as r:\n",
    "            r.raise_for_status()\n",
    "            df = pd.read_csv(BytesIO(r.content), header=None, dtype=str, sep=' ', names=['names', 'type', 'length', 'declength'], encoding=encoding)\n",
    "\n",
    "        for file in table_files:\n",
    "            try:\n",
    "                url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets/{bucket_id}/files?path=/' + file\n",
    "                header_json = {'Authorization': 'AppKey {}'.format(appkey), 'Accept': 'application/octet-stream'}\n",
    "                with requests.get(url, headers=header_json, stream=False) as r:\n",
    "                    r.raise_for_status()\n",
    "                    if 'HEADER' in file:\n",
    "                        continue\n",
    "                    elif file.split('.')[-1] == 'zip':\n",
    "                        z = zipfile.ZipFile(BytesIO(r.content))\n",
    "                        fh = z.open(z.infolist()[0])\n",
    "                    else:\n",
    "                        fh = BytesIO(r.content)\n",
    "                    df_up = pd.read_csv(fh, header=None, dtype='string', sep=';', names=list(df['names']), quotechar='\"', encoding=encoding)\n",
    "                    buffer = BytesIO()\n",
    "                    df_up.to_parquet(buffer, index=False, compression='snappy')\n",
    "                    uppie.push_new_chunk(pool_id=poolid, job_id=jobhandle['id'], file_path=buffer.getvalue())\n",
    "            except Exception as e:\n",
    "                logging.error(f'{file} failed with error: {e}')\n",
    "        uppie.submit_job(pool_id=poolid, job_id=jobhandle['id'])\n",
    "logging.info('upload done.')\n",
    "running = True\n",
    "while running:\n",
    "    jobs = uppie.list_jobs(poolid)\n",
    "    for jobids in jobstatus:\n",
    "        for i in jobs:\n",
    "            try:\n",
    "                if i['id'] == jobids:\n",
    "                    if i['status'] == 'QUEUED':\n",
    "                        pass\n",
    "                    elif jobstatus[jobids] is True:\n",
    "                        pass\n",
    "                    elif i['status'] == 'DONE':\n",
    "                        jobstatus[jobids] = True\n",
    "                    elif i['status'] != 'RUNNING':\n",
    "                        jobstatus[jobids] = True\n",
    "                    else:\n",
    "                        pass\n",
    "                    break\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                logging.error('terminating program\\n')\n",
    "                quit()\n",
    "            except:\n",
    "                pass\n",
    "    if all(status is True for status in jobstatus.values()):\n",
    "        running = False\n",
    "        for i in jobs:\n",
    "            if i['id'] in jobstatus:\n",
    "                if i['status'] == 'DONE':\n",
    "                    logging.info(f\"{i['targetName']} was successfully installed in the database\")\n",
    "                else:\n",
    "                    logging.error(f\"{i['targetName']} failed with: {i}\")\n",
    "    else:\n",
    "        time.sleep(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
