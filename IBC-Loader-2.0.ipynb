{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pycelonis import get_celonis\n",
    "import fastparquet as fp\n",
    "import pyarrow as pa\n",
    "import time\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "class cloud:\n",
    "\n",
    "    def get_api(self, path):\n",
    "        return \"https://{}.{}.celonis.cloud/{}\".format(self.tenant, self.realm,\n",
    "                                                       path)\n",
    "\n",
    "    def __init__(self, tenant, realm, api_key):\n",
    "        self.tenant = tenant\n",
    "        self.realm = realm\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def get_jobs_api(self, pool_id):\n",
    "        return self.get_api(\"integration/api/v1/data-push/{}/jobs/\"\n",
    "                            .format(pool_id))\n",
    "\n",
    "    def get_auth(self):\n",
    "        return {'authorization': \"Bearer {}\".format(self.api_key)}\n",
    "\n",
    "    def list_jobs(self, pool_id):\n",
    "        api = self.get_jobs_api(pool_id)\n",
    "        return requests.get(api, headers=self.get_auth()).json()\n",
    "\n",
    "    def delete_job(self, pool_id, job_id):\n",
    "        api = self.get_jobs_api(pool_id) + \"/{}\".format(job_id)\n",
    "        return requests.delete(api, headers=self.get_auth())\n",
    "\n",
    "    def create_job(self, pool_id, targetName, data_connection_id,\n",
    "                   upsert=False):\n",
    "        api = self.get_jobs_api(pool_id)\n",
    "        job_type = \"REPLACE\"\n",
    "        if upsert:\n",
    "            job_type = \"DELTA\"\n",
    "        if not data_connection_id:\n",
    "            payload = {'targetName': targetName, 'type': job_type,\n",
    "                       'dataPoolId': pool_id}\n",
    "        else:\n",
    "            payload = {'targetName': targetName, 'type': job_type,\n",
    "                       'dataPoolId': pool_id,\n",
    "                       'connectionId': data_connection_id}\n",
    "        return requests.post(api, headers=self.get_auth(), json=payload).json()\n",
    "\n",
    "    def push_new_dir(self, pool_id, job_id, dir_path):\n",
    "        files = [join(dir_path, f) for f in listdir(dir_path)\n",
    "                 if isfile(join(dir_path, f))]\n",
    "        parquet_files = list(filter(lambda f: f.endswith(\".parquet\"), files))\n",
    "        for parquet_file in parquet_files:\n",
    "            logging.debug(f\"Uploading chunk {parquet_file}\")\n",
    "            self.push_new_chunk(pool_id, job_id, parquet_file)\n",
    "\n",
    "    def push_new_chunk(self, pool_id, job_id, file_path):\n",
    "        api = self.get_jobs_api(pool_id) + \"/{}/chunks/upserted\".format(job_id)\n",
    "        upload_file = {\"file\": file_path}\n",
    "        return requests.post(api, files=upload_file, headers=self.get_auth())\n",
    "\n",
    "    def submit_job(self, pool_id, job_id):\n",
    "        api = self.get_jobs_api(pool_id) + \"/{}/\".format(job_id)\n",
    "        return requests.post(api, headers=self.get_auth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the following three parameters for your team\n",
    "team = '|team|'\n",
    "realm = '|realm|'\n",
    "poolid = '|poolid|'\n",
    "appkey = '|appkey|'\n",
    "apikey = '|apikey|'\n",
    "connectionid = None\n",
    "delta = False\n",
    "\n",
    "url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets?feature=SFTP'\n",
    "header_json = {'Authorization': f'AppKey {appkey}', 'Accept': 'application/json'}\n",
    "file_response = requests.get(url, headers=header_json)\n",
    "buckets = [i['id'] for i in file_response.json()]\n",
    "for bucket_id in buckets:\n",
    "    print(bucket_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set bucket_id if you leave this blank a random id will be chosen\n",
    "bucket_id = ''\n",
    "\n",
    "if bucket_id == '':\n",
    "    for bucketed_id in buckets:\n",
    "        bucket_id = bucketed_id\n",
    "\n",
    "\n",
    "#set folder to scan (default is the root folder)\n",
    "path_to_file = ''\n",
    "encoding = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the file desciptions\n",
    "url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets?feature=SFTP'\n",
    "header_json = {'Authorization': f'AppKey {appkey}', 'Accept': 'application/json'}\n",
    "file_response = requests.get(url, headers=header_json)\n",
    "# get all the file names\n",
    "\n",
    "url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets/{bucket_id}/files?path=/' + path_to_file\n",
    "header_json = {'Authorization': f'AppKey {appkey}', 'Accept': 'application/json'}\n",
    "file_response = requests.get(url, headers=header_json)\n",
    "files = []\n",
    "for i in file_response.json()['children']:\n",
    "    if i['type'] == 'FILE':\n",
    "         files.append(i['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with all the headers in it that are most recent\n",
    "header = sorted([header for header in files if ('_HEADER' in header)], reverse=True)\n",
    "header2 = [header[0]]\n",
    "for i in header:\n",
    "    if all(i.split('HEADER')[0] not in h for h in header2):\n",
    "        header2.append(i)\n",
    "header = header2\n",
    "\n",
    "jobstatus = {}\n",
    "uppie = cloud(tenant=team, realm=realm, api_key=apikey)\n",
    "\n",
    "# start the upload per header\n",
    "for i in header:\n",
    "    if True:\n",
    "        print(i)\n",
    "        table_files = []\n",
    "        indices = []\n",
    "        ref = i.split('HEADER')\n",
    "        ref[1] = ref[1].replace('.csv', '')\n",
    "        targetname = ref[0][:-1]\n",
    "        jobhandle = uppie.create_job(pool_id=poolid,\n",
    "                             data_connection_id=connectionid,\n",
    "                             targetName=targetname,\n",
    "                             upsert=delta)\n",
    "        jobstatus[jobhandle['id']] = False\n",
    "        for n in range(len(files)):\n",
    "            if ref[0] in files[n] and ref[1] in files[n]:\n",
    "                indices.append(n)\n",
    "        for m in indices[::-1]:\n",
    "            table_files.append(files.pop(m))\n",
    "        url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets/{bucket_id}/files?path=/' + i\n",
    "        header_json = {'Authorization': 'AppKey {}'.format(appkey), 'Accept': 'application/octet-stream'}\n",
    "        with requests.get(url, headers=header_json, stream=False) as r:\n",
    "            r.raise_for_status()\n",
    "            df = pd.read_csv(BytesIO(r.content), header=None, dtype=str, sep=' ', names=['names', 'type', 'length', 'declength'], encoding=encoding)\n",
    "\n",
    "        for file in table_files:\n",
    "            try:\n",
    "                url = f'https://{team}.{realm}.celonis.cloud/storage-manager/api/buckets/{bucket_id}/files?path=/' + file\n",
    "                header_json = {'Authorization': 'AppKey {}'.format(appkey), 'Accept': 'application/octet-stream'}\n",
    "                with requests.get(url, headers=header_json, stream=False) as r:\n",
    "                    r.raise_for_status()\n",
    "                    if 'HEADER' in file:\n",
    "                        continue\n",
    "                    elif file.split('.')[-1] == 'zip':\n",
    "                        z = zipfile.ZipFile(BytesIO(r.content))\n",
    "                        fh = z.open(z.infolist()[0])\n",
    "                    else:\n",
    "                        fh = BytesIO(r.content)\n",
    "                    df_up = pd.read_csv(fh, header=None, dtype='string', sep=';', names=list(df['names']), quotechar='\"', encoding=encoding)\n",
    "                    buffer = BytesIO()\n",
    "                    df_up.to_parquet(buffer, index=False, compression='snappy')\n",
    "                    uppie.push_new_chunk(pool_id=poolid, job_id=jobhandle['id'], file_path=buffer.getvalue())\n",
    "            except Exception as e:\n",
    "                print(f'{file} failed with error: {e}')\n",
    "        uppie.submit_job(pool_id=poolid, job_id=jobhandle['id'])\n",
    "print('upload done.')\n",
    "running = True\n",
    "while running:\n",
    "    jobs = uppie.list_jobs(poolid)\n",
    "    for jobids in jobstatus:\n",
    "        for i in jobs:\n",
    "            try:\n",
    "                if i['id'] == jobids:\n",
    "                    if i['status'] == 'QUEUED':\n",
    "                        pass\n",
    "                    elif jobstatus[jobids] is True:\n",
    "                        pass\n",
    "                    elif i['status'] == 'DONE':\n",
    "                        jobstatus[jobids] = True\n",
    "                    elif i['status'] != 'RUNNING':\n",
    "                        jobstatus[jobids] = True\n",
    "                    else:\n",
    "                        pass\n",
    "                    break\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                print('terminating program\\n')\n",
    "                quit()\n",
    "            except:\n",
    "                pass\n",
    "    if all(status is True for status in jobstatus.values()):\n",
    "        running = False\n",
    "        for i in jobs:\n",
    "            if i['id'] in jobstatus:\n",
    "                if i['status'] == 'DONE':\n",
    "                    print(f\"{i['targetName']} was successfully installed in the database\")\n",
    "                else:\n",
    "                    print(f\"{i['targetName']} failed with: {i}\")\n",
    "    else:\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs:\n",
    "    if i['id'] in jobstatus:\n",
    "        if i['status'] == 'DONE':\n",
    "            print(f\"{i['targetName']} was successfully installed in the database\")\n",
    "        else:\n",
    "            print(f\"{i['targetName']} failed with: {i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
