{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the target url and if you want to do a delta load\n",
    "from cloud_upload_config import *\n",
    "\n",
    "if as_string == 'all string':\n",
    "    as_string = True\n",
    "else:\n",
    "    as_string = False\n",
    "\n",
    "if delta == 'full':\n",
    "    delta = False\n",
    "else:\n",
    "    delta = True\n",
    "    \n",
    "if exclude_loaded != 'skip':\n",
    "    exclude_loaded = False\n",
    "else:\n",
    "    exclude_loaded = True\n",
    "\n",
    "local_file_path = '/home/jovyan/localfiles/'\n",
    "look_for_sap_files_globally = False\n",
    "path_to_folder = ''\n",
    "if local_file_path is not None:\n",
    "    path_to_folder = local_file_path\n",
    "continue_from_last_time = False\n",
    "# this determines how detailed the log is, where INFO is the standard. the list below is ordered from most detailed (DEBUG) to least detailled (CRITICAL)\n",
    "# logging.DEBUG\n",
    "# logging.INFO\n",
    "# logging.WARNING\n",
    "# logging.ERROR\n",
    "# logging.CRITICAL\n",
    "#log_level = logging.DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global compressed\n",
    "compressed = ['.tar', '.gz', '.zip', '.7z']\n",
    "\n",
    "global generic_file_type\n",
    "generic_file_type = ['.csv', '.xlsx', '.xls']\n",
    "\n",
    "global sap_file_type\n",
    "sap_file_type = '(.*)_[0-9]{8}_[0-9]{6}.'\n",
    "\n",
    "global encrypted\n",
    "encrypted = ['.gpg', '.pgp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "#logname = f'IBC_Loader_log_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log'\n",
    "FORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
    "formatter = logging.Formatter(FORMAT)\n",
    "logging.basicConfig(format=FORMAT, filename=logname, level=logging.INFO)\n",
    "print(logname)\n",
    "logging.info('logging initialized')\n",
    "\n",
    "try:\n",
    "    import re\n",
    "    import subprocess\n",
    "    import json\n",
    "    import requests\n",
    "    import py7zr\n",
    "    import tarfile\n",
    "    from io import BytesIO\n",
    "    import zipfile\n",
    "    import gzip\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from multiprocessing import Pool\n",
    "    from pycelonis import get_celonis\n",
    "    from chardet.universaldetector import UniversalDetector\n",
    "    import pycelonis\n",
    "    import fastparquet as fp\n",
    "    import pyarrow as pa\n",
    "    import time\n",
    "    import itertools\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    from pathlib import Path\n",
    "    from itertools import product\n",
    "    import copy\n",
    "    import sys\n",
    "    #from lib.cloud_module import cloud\n",
    "    #from lib.upload_module import import_sap_header\n",
    "except ModuleNotFoundError as e:\n",
    "    logging.error(e)\n",
    "    logging.error('please install missing packages to use this program.')\n",
    "    logging.error('shutting down')\n",
    "    quit()\n",
    "\n",
    "if agreed != 'yes':\n",
    "    logging.error('you need to read and accept the terms listed in disclaimer.md')\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cloud:\n",
    "\n",
    "    def get_api(self, path):\n",
    "        return f\"https://{self.tenant}.{self.realm}.celonis.cloud/{path}\"\n",
    "\n",
    "    def __init__(self, tenant, realm, api_key):\n",
    "        self.tenant = tenant\n",
    "        self.realm = realm\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def get_jobs_api(self, pool_id):\n",
    "        return self.get_api(f\"integration/api/v1/data-push/{pool_id}/jobs/\")\n",
    "\n",
    "    def get_auth(self):\n",
    "        return {'authorization': f\"AppKey {self.api_key}\"}\n",
    "\n",
    "    def list_jobs(self, pool_id):\n",
    "        api = self.get_jobs_api(pool_id)\n",
    "        return requests.get(api, headers=self.get_auth()).json()\n",
    "\n",
    "    def delete_job(self, pool_id, job_id):\n",
    "        api = self.get_jobs_api(pool_id) + f\"/{job_id}\"\n",
    "        return requests.delete(api, headers=self.get_auth())\n",
    "\n",
    "    def create_job(self, pool_id, targetName, data_connection_id,\n",
    "                   upsert=False):\n",
    "        api = self.get_jobs_api(pool_id)\n",
    "        job_type = \"REPLACE\"\n",
    "        if upsert:\n",
    "            job_type = \"DELTA\"\n",
    "        if not data_connection_id:\n",
    "            payload = {'targetName': targetName, 'type': job_type,\n",
    "                       'dataPoolId': pool_id}\n",
    "        else:\n",
    "            payload = {'targetName': targetName, 'type': job_type,\n",
    "                       'dataPoolId': pool_id,\n",
    "                       'connectionId': data_connection_id}\n",
    "        r = requests.post(api, headers=self.get_auth(), json=payload)\n",
    "        logging.debug(f'created job with {r}')\n",
    "        return r.json()\n",
    "\n",
    "    def push_new_dir(self, pool_id, job_id, dir_path):\n",
    "        files = [join(dir_path, f) for f in listdir(dir_path)\n",
    "                 if isfile(join(dir_path, f))]\n",
    "        parquet_files = list(filter(lambda f: f.endswith(\".parquet\"), files))\n",
    "        for parquet_file in parquet_files:\n",
    "            logging.debug(f\"Uploading chunk {parquet_file}\")\n",
    "            self.push_new_chunk(pool_id, job_id, parquet_file)\n",
    "\n",
    "    def push_new_chunk(self, pool_id, job_id, file_path):\n",
    "        api = self.get_jobs_api(pool_id) + f\"/{job_id}/chunks/upserted\"\n",
    "        upload_file = {\"file\": file_path}\n",
    "        r = requests.post(api, files=upload_file, headers=self.get_auth())\n",
    "        logging.debug(f'pushed new chunk with {r}')\n",
    "        return r\n",
    "\n",
    "    def submit_job(self, pool_id, job_id):\n",
    "        api = self.get_jobs_api(pool_id) + f\"/{job_id}\"\n",
    "        r = requests.post(api, headers=self.get_auth())\n",
    "        logging.debug(f'submitted job {r}')\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_tables_loaded(ibc_team):\n",
    "    # Create new table with the subset we're interested in\n",
    "    data = None\n",
    "    celonis = get_celonis()\n",
    "    logging.info('checking for tables that have already been loaded.')\n",
    "    random_name = f'zzz___TEMP___{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "    \n",
    "    if ibc_team.connectionid is None:\n",
    "        create_table_from_query_statement = f'CREATE TABLE IF NOT EXISTS \"{random_name}\" AS (SELECT table_name FROM tables WHERE table_schema = \\'{ibc_team.poolid}\\');'\n",
    "    else:\n",
    "        create_table_from_query_statement = f'CREATE TABLE IF NOT EXISTS \"{random_name}\" AS (SELECT table_name FROM tables WHERE table_schema = \\'{ibc_team.poolid}_{ibc_team.connectionid}\\');'\n",
    "    # table_name FROM tables where table_schema = \\'\\'\n",
    "    \n",
    "    # Create data job and run table creation script\n",
    "    p = celonis.pools.find(ibc_team.poolid)\n",
    "    counter = 0\n",
    "    while counter < 4:\n",
    "        counter += 1\n",
    "        try:\n",
    "            dj = p.create_data_job(random_name)\n",
    "            transf = dj.create_transformation(random_name, create_table_from_query_statement)\n",
    "            transf.execute()\n",
    "\n",
    "            # Create temporary data model in pool and add recently created table, then reload\n",
    "            dm = p.create_datamodel(random_name)\n",
    "            try:\n",
    "                dm.add_tables_from_pool(random_name)\n",
    "                dm.reload(from_cache=False, wait_for_reload=True)\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Find table object in data model and download\n",
    "                t = dm.tables.find(random_name)\n",
    "\n",
    "                path = t._get_data_file(Path('.') / random_name)\n",
    "                data = pd.read_parquet(path)\n",
    "            except Exception as e:\n",
    "                logging.error(f'determining what tables have been loaded failed with: {e}')\n",
    "            finally:\n",
    "                # Deleting temporary objects\n",
    "                dm.delete()\n",
    "                transf.statement = f'DROP TABLE IF EXISTS \"{random_name}\";'\n",
    "                transf.execute()\n",
    "                dj.delete()\n",
    "                if sys.version_info > (3,8):\n",
    "                    path.unlink(missing_ok=True)\n",
    "                else:\n",
    "                    path.unlink()\n",
    "            loaded_tables = pd.Series(data['table_name']).tolist()\n",
    "            try:\n",
    "                loaded_tables.remove(random_name)\n",
    "            except:\n",
    "                pass\n",
    "            logging.info(f'these tables are already in the data pool: {loaded_tables}')\n",
    "            break\n",
    "        except:\n",
    "            loaded_tables = []\n",
    "        logging.warning(f'determine_tables_loaded failed for {counter}. time. Retrying.')\n",
    "        time.sleep(1)\n",
    "    return loaded_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_line_count_of_loaded_tables(ibc_team):\n",
    "    # Create new table with the subset we're interested in\n",
    "    data = None\n",
    "    celonis = get_celonis()\n",
    "    logging.info('counting lines of tables that have been loaded.')\n",
    "    \n",
    "    random_name = f'zzz___TEMP_LC___{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "    add_line_counts_statement = []\n",
    "    add_line_counts_statement.append(f'CREATE TABLE IF NOT EXISTS \"{random_name}\" (\"TABLE\" VARCHAR(80), \"COUNT\" INTEGER);\\n')\n",
    "    \n",
    "    tables = determine_tables_loaded(ibc_team)\n",
    "    \n",
    "    \n",
    "    if ibc_team.connectionid is None:\n",
    "        for t in tables:\n",
    "            add_line_counts_statement.append(f'INSERT INTO \"{random_name}\" (\"TABLE\" ,\"COUNT\") SELECT \\'{t}\\', COUNT(1) FROM \"{t}\";\\n')\n",
    "    else:\n",
    "        for t in tables:\n",
    "            add_line_counts_statement.append(f'INSERT INTO \"{random_name}\" (\"TABLE\" ,\"COUNT\") SELECT \\'{t}\\', COUNT(1) FROM <%=DATASOURCE:JDBC%>.\"{t}\";\\n')\n",
    "    add_line_counts_statement = ''.join(add_line_counts_statement)\n",
    "    \n",
    "    # table_name FROM tables where table_schema = \\'\\'\n",
    "    # Create data job and run table creation script\n",
    "    p = celonis.pools.find(ibc_team.poolid)\n",
    "    counter = 0\n",
    "    while counter < 4:\n",
    "        counter += 1\n",
    "        try:\n",
    "            dj = p.create_data_job(random_name)\n",
    "            transf = dj.create_transformation(random_name, add_line_counts_statement)\n",
    "            transf.execute(wait_for_execution=True)\n",
    "            # Create temporary data model in pool and add recently created table, then reload\n",
    "            dm = p.create_datamodel(random_name)\n",
    "            try:\n",
    "                dm.add_tables_from_pool(random_name)\n",
    "                dm.reload(from_cache=False, wait_for_reload=True)\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Find table object in data model and download\n",
    "                t = dm.tables.find(random_name)\n",
    "\n",
    "                path = t._get_data_file(Path('.') / random_name)\n",
    "                data = pd.read_parquet(path)\n",
    "            except Exception as e:\n",
    "                logging.error(f'determining line count per table failed with: {e}')\n",
    "            finally:\n",
    "                # Deleting temporary objects\n",
    "                dm.delete()\n",
    "                transf.statement = f'DROP TABLE IF EXISTS \"{random_name}\";'\n",
    "                transf.execute()\n",
    "                dj.delete()\n",
    "                if sys.version_info > (3,8):\n",
    "                    path.unlink(missing_ok=True)\n",
    "                else:\n",
    "                    path.unlink()\n",
    "            #logging.info(f'these tables are already in the data pool: {loaded_tables}')\n",
    "            break\n",
    "        except:\n",
    "            loaded_tables = []\n",
    "        logging.warning(f'determine_tables_loaded failed for {counter}. time. Retrying.')\n",
    "        time.sleep(1)\n",
    "    return data\n",
    "    #data.to_excel('lines.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table_name(name):\n",
    "    name = name.replace('.', '/')\n",
    "    forbidden = ['|', ' ', ',']\n",
    "    for f in forbidden:\n",
    "        name = name.replace(f, '_')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const(n):\n",
    "    yield n\n",
    "    yield from const(n)\n",
    "    \n",
    "def decider(indicator, value):\n",
    "    if indicator == 'float':\n",
    "        return flt(value)\n",
    "    elif indicator == 'int':\n",
    "        return inti(value)\n",
    "    elif indicator == 'time':\n",
    "        return time_casting(value)\n",
    "    else:\n",
    "        return dt(value)\n",
    "\n",
    "def time_casting(df):\n",
    "    try:\n",
    "        df = '19700101'+df\n",
    "        return dt(df)\n",
    "    except Exception as e:\n",
    "        logging.error(f'casting {df} to time failed with {e}')\n",
    "\n",
    "def dt(df):\n",
    "    try:\n",
    "        df = pd.to_datetime(df, errors='coerce')\n",
    "        return df.astype('datetime64')\n",
    "    except Exception as e:\n",
    "        logging.error(f'casting {df} to datetime failed with {e}')\n",
    "\n",
    "def flt(df):\n",
    "    try:\n",
    "        mask = df.str.contains('-')\n",
    "        df = df.str.replace('-', '').str.strip()\n",
    "        df = pd.to_numeric(df, downcast='float', errors='coerce')\n",
    "        #df = df.astype(float)\n",
    "        return df.mask(mask, -df)\n",
    "    except Exception as e:\n",
    "        logging.error(f'casting {df} to float failed with {e}')\n",
    "\n",
    "def inti(df):\n",
    "    try:\n",
    "        mask = df.str.contains('-')\n",
    "        df = df.str.replace('-', '').str.strip()\n",
    "        df = pd.to_numeric(df, downcast='signed', errors='coerce')\n",
    "        #df = df.str.replace('-', '').str.strip().astype('int64')\n",
    "        #df = df.astype(int)\n",
    "        return df.mask(mask, -df)\n",
    "    except Exception as e:\n",
    "        logging.error(f'casting {df} to int failed with {e}')\n",
    "\n",
    "def type_determination(x):\n",
    "    if x in ['CURR', 'QUAN', 'DEC', 'FLTP']:\n",
    "        return 'float'\n",
    "    elif x in ['INT1', 'INT2', 'INT4', 'PREC']:\n",
    "        return 'int'\n",
    "    elif x in ['DATS']:\n",
    "        return 'date'\n",
    "    elif x in ['TIMS']:\n",
    "        return 'time'\n",
    "    else:\n",
    "        return 'str'\n",
    "\n",
    "def sap_load(lst, pwd=None):\n",
    "    #zip(relevant_files, const(pre_url), const(header_json), const(header), const(jobhandle['id']))\n",
    "    file = lst[0]\n",
    "    pre_url = lst[1]\n",
    "    header_json = lst[2]\n",
    "    header = lst[3]\n",
    "    job_id = lst[4]\n",
    "    encoding = 'utf-8'\n",
    "    df = lst[5]\n",
    "    type_dict = lst[6]\n",
    "    \n",
    "    try:\n",
    "        logging.info(f'uploading chunk: {file.file}')\n",
    "        if pre_url is not None:\n",
    "            url = pre_url + file.file\n",
    "            with requests.get(url, headers=header_json, stream=False) as r:\n",
    "                r.raise_for_status()\n",
    "                if 'HEADER' in file.file:\n",
    "                    return\n",
    "                elif file.file.split('.')[-1] == 'zip':\n",
    "                    z = zipfile.ZipFile(BytesIO(r.content))\n",
    "                    fh = z.open(z.infolist()[0])\n",
    "                elif file.file.split('.')[-1] == '7z':\n",
    "                    z = py7zr.SevenZipFile(BytesIO(r.content), password=pwd)\n",
    "                    filename = z.getnames()[0]\n",
    "                    fh = z.read(filename)[filename]\n",
    "                    logging.info(fh)\n",
    "                elif file.file.split('.')[-1] == 'gz':\n",
    "                    fh = gzip.GzipFile(fileobj=BytesIO(r.content), mode='rb')\n",
    "                else:\n",
    "                    fh = BytesIO(r.content)\n",
    "                df_up = pd.read_csv(fh, header=None, dtype='string', sep=';', names=list(df['names']), quotechar='\"', encoding=encoding, escapechar='\\\\')\n",
    "        else:\n",
    "            if 'HEADER' in str(file.file):\n",
    "                return\n",
    "            elif str(file.file).split('.')[-1] == 'zip':\n",
    "                z = zipfile.ZipFile(file.file)\n",
    "                fh = z.open(z.infolist()[0])\n",
    "            elif str(file.file).split('.')[-1] == '7z':\n",
    "                z = py7zr.SevenZipFile(file.file, password=pwd)\n",
    "                filename = z.getnames()[0]\n",
    "                fh = z.read(filename)[filename]\n",
    "                logging.info(fh)\n",
    "            elif str(file.file).split('.')[-1] == 'gz':\n",
    "                fh = gzip.GzipFile(file.file, mode='rb')\n",
    "            else:\n",
    "                fh = str(file.file)\n",
    "            df_up = pd.read_csv(fh, header=None, dtype='string', sep=';', names=list(df['names']), quotechar='\"', encoding=encoding, escapechar='\\\\')\n",
    "        if len(type_dict) > 0:\n",
    "            for i in type_dict:\n",
    "                df_up[type_dict[i]] = df_up[type_dict[i]].apply(lambda x: decider(i, x), axis=0)\n",
    "                logging.debug(f'conversion to {i} resulted in {df_up[type_dict[i]].dtypes.value_counts()}')\n",
    "        buffer = BytesIO()\n",
    "        df_up.to_parquet(buffer, index=False, compression='snappy', use_deprecated_int96_timestamps=True, version='2.0')\n",
    "        uppie.push_new_chunk(pool_id=header.poolid, job_id=job_id, file_path=buffer.getvalue())\n",
    "    except Exception as e:\n",
    "        logging.error(f'{file.file} failed with error: {e}')\n",
    "        raise\n",
    "\n",
    "def import_sap_header(header, files, jobstatus, uppie, data, location_indicator='local', delta=False, pwd=None):\n",
    "    try:\n",
    "        encoding = 'utf-8'\n",
    "        \"\"\"\n",
    "        team: {self.team}\n",
    "                        realm: {self.realm}\n",
    "                        poolid: {self.poolid}\n",
    "                        connectionid: {self.connectionid}\n",
    "                        appkey: {self.appkey}\n",
    "                        apikey: {self.apikey}\n",
    "                        url: {self.url}\n",
    "        \"\"\"\n",
    "        if location_indicator == 'local':\n",
    "            ref = str(header.file).split('HEADER')\n",
    "            ref[1] = ref[1].replace('.csv', '')\n",
    "        elif location_indicator == 'global':\n",
    "            ref = Path(header.file).name.split('HEADER')\n",
    "            ref[1] = ref[1].replace('.csv', '')\n",
    "        else:\n",
    "            logging.error(f'location indicator {location_indicator} is invalid')\n",
    "            raise ValueError(f'location indicator {location_indicator} is invalid')\n",
    "        targetname = Path(ref[0][:-1]).name #.replace(path_to_file, '')\n",
    "        targetname = clean_table_name(targetname)\n",
    "        if targetname in data:\n",
    "            logging.warning(f'skipping {header.file} as table with {targetname} is already present in target pool.')\n",
    "            return None\n",
    "        jobhandle = uppie.create_job(pool_id=header.poolid,\n",
    "                             data_connection_id=header.connectionid,\n",
    "                             targetName=targetname,\n",
    "                             upsert=delta)\n",
    "        logging.info(f'starting to upload {targetname}')\n",
    "        logging.debug(jobhandle)\n",
    "        jobstatus[jobhandle['id']] = False\n",
    "        if header.bucket_id is not None:\n",
    "            url = f'https://{header.team}.{header.realm}.celonis.cloud/storage-manager/api/buckets/{header.bucket_id}/files?path=/' + header.file\n",
    "            header_json = {'Authorization': 'AppKey {}'.format(header.appkey), 'Accept': 'application/octet-stream'}\n",
    "            with requests.get(url, headers=header_json, stream=False) as r:\n",
    "                r.raise_for_status()\n",
    "                df = pd.read_csv(BytesIO(r.content), header=None, dtype=str, sep=' ', names=['names', 'type', 'length', 'declength'], encoding=encoding)\n",
    "        else:\n",
    "            df = pd.read_csv(str(header.file), header=None, dtype=str, sep=' ', names=['names', 'type', 'length', 'declength'], encoding=encoding)\n",
    "        df['type'] = df['type'].apply(lambda x: type_determination(x))\n",
    "        type_dict = {}\n",
    "        if len(df[df['type'] == 'float']) > 0:\n",
    "            type_dict['float'] = list(df[df['type'] == 'float']['names'])\n",
    "        if len(df[df['type'] == 'int']) > 0:\n",
    "            type_dict['int'] = list(df[df['type'] == 'int']['names'])\n",
    "        if len(df[df['type'] == 'date']) > 0:\n",
    "            type_dict['date'] = list(df[df['type'] == 'date']['names'])\n",
    "        if len(df[df['type'] == 'time']) > 0:\n",
    "            type_dict['time'] = list(df[df['type'] == 'time']['names'])\n",
    "        relevant_files = tuple(f for f in files if ref[0] in str(f.file) and ref[1] in str(f.file))\n",
    "        if header.bucket_id is not None:\n",
    "            header_json = {'Authorization': 'AppKey {}'.format(header.appkey), 'Accept': 'application/octet-stream'}\n",
    "            pre_url = f'https://{header.team}.{header.realm}.celonis.cloud/storage-manager/api/buckets/{header.bucket_id}/files?path=/'\n",
    "            with Pool() as pool:\n",
    "                pool.map(sap_load, zip(relevant_files, const(pre_url), const(header_json), const(header), const(jobhandle['id']), const(df), const(type_dict)))\n",
    "        else:\n",
    "            with Pool() as pool:\n",
    "                pool.map(sap_load, zip(relevant_files, const(None), const(None), const(header), const(jobhandle['id']), const(df), const(type_dict)))\n",
    "        uppie.submit_job(pool_id=header.poolid, job_id=jobhandle['id'])\n",
    "    except Exception as e:\n",
    "        logging.error(f'importing sap file failed with {e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(non_sap_file, pwd):\n",
    "    logging.info(f'starting encoding determination')\n",
    "    detector = UniversalDetector()\n",
    "    detector.reset()\n",
    "    #counter = 0\n",
    "    try:\n",
    "        \"\"\"\n",
    "        with open(file, 'rb') as file_detect:\n",
    "        \"\"\"\n",
    "        url = f'https://{non_sap_file.team}.{non_sap_file.realm}.celonis.cloud/storage-manager/api/buckets/{non_sap_file.bucket_id}/files?path=/' + non_sap_file.file\n",
    "        header_json = {'Authorization': 'AppKey {}'.format(non_sap_file.appkey), 'Accept': 'application/octet-stream'}\n",
    "        with requests.get(url, headers=header_json, stream=True) as file_detect:\n",
    "            file_detect.raise_for_status()\n",
    "            if non_sap_file.file.split('.')[-1] == 'zip':\n",
    "                z = zipfile.ZipFile(BytesIO(file_detect.content))\n",
    "                zip_content = z.infolist()\n",
    "                idx = 0\n",
    "                for c, zips in enumerate(zip_content):\n",
    "                    if '.csv' in zips.filename:\n",
    "                        idx = c\n",
    "                        break\n",
    "                logging.debug(zip_content[idx])\n",
    "                fh = z.open(zip_content[idx])\n",
    "            elif non_sap_file.file.split('.')[-1] == '7z':\n",
    "                z = py7zr.SevenZipFile(BytesIO(file_detect.content), password=pwd)\n",
    "                filename = z.getnames()[0]\n",
    "                fh = z.read(filename)[filename]\n",
    "                logging.debug(fh)\n",
    "            elif non_sap_file.file.split('.')[-2] == 'tar':\n",
    "                fh = tarfile.open(fileobj=BytesIO(file_detect.content), mode='r:gz')\n",
    "            elif non_sap_file.file.split('.')[-1] == 'gz':\n",
    "                fh = gzip.GzipFile(fileobj=BytesIO(file_detect.content), mode='rb')\n",
    "            else:\n",
    "                fh = BytesIO(file_detect.content)\n",
    "            for counter, line in enumerate(fh):\n",
    "                #counter += 1\n",
    "                detector.feed(line)\n",
    "                if detector.done:\n",
    "                    break\n",
    "                elif counter > 50000:\n",
    "                    break\n",
    "        detector.close()\n",
    "        enc = detector.result['encoding'].lower()\n",
    "        logging.info(f'{non_sap_file.file} has encoding: {detector.result}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'encoding detection failed with: {e}\\nreverting to utf-8 as standard')\n",
    "        enc = 'utf-8'\n",
    "    return enc\n",
    "\n",
    "def import_non_sap_file(non_sap_file, jobstatus, uppie, data, pwd=None, delta=False, as_string=True):\n",
    "    encoding = 'utf-8'\n",
    "    \"\"\"\n",
    "    team: {self.team}\n",
    "                    realm: {self.realm}\n",
    "                    poolid: {self.poolid}\n",
    "                    connectionid: {self.connectionid}\n",
    "                    appkey: {self.appkey}\n",
    "                    apikey: {self.apikey}\n",
    "                    url: {self.url}\n",
    "\n",
    "    if non_sap_file.size > 350 * 1024 * 1024:\n",
    "        logging.warning(f'skipped file {non_sap_file.file} because it was too large.')\n",
    "        return None#\n",
    "    \"\"\"\n",
    "    targetname = Path(non_sap_file.file).name.split('.')[0]\n",
    "    targetname = clean_table_name(targetname)\n",
    "    if targetname in data:\n",
    "        logging.warning(f'skipping {non_sap_file.file} as table with {targetname} is already present in target pool.')\n",
    "        return None\n",
    "    jobhandle = uppie.create_job(pool_id=non_sap_file.poolid,\n",
    "                         data_connection_id=non_sap_file.connectionid,\n",
    "                         targetName=targetname,\n",
    "                         upsert=delta)\n",
    "    logging.info(f'starting to upload {targetname}')\n",
    "    logging.debug(jobhandle)\n",
    "    jobstatus[jobhandle['id']] = False\n",
    "    encoding = detect_encoding(non_sap_file, pwd)\n",
    "    try:\n",
    "        url = f'https://{non_sap_file.team}.{non_sap_file.realm}.celonis.cloud/storage-manager/api/buckets/{non_sap_file.bucket_id}/files?path=/' + non_sap_file.file\n",
    "        header_json = {'Authorization': 'AppKey {}'.format(non_sap_file.appkey), 'Accept': 'application/octet-stream'}\n",
    "        with requests.get(url, headers=header_json, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            if non_sap_file.file.split('.')[-1] == 'zip':\n",
    "                z = zipfile.ZipFile(BytesIO(r.content))\n",
    "                zip_content = z.infolist()\n",
    "                idx = 0\n",
    "                for c, zips in enumerate(zip_content):\n",
    "                    if '.csv' in zips.filename:\n",
    "                        idx = c\n",
    "                        break\n",
    "                logging.debug(zip_content[idx])\n",
    "                fh = z.open(zip_content[idx])\n",
    "            elif non_sap_file.file.split('.')[-1] == '7z':\n",
    "                z = py7zr.SevenZipFile(BytesIO(r.content), password=pwd)\n",
    "                filename = z.getnames()[0]\n",
    "                fh = z.read(filename)[filename]\n",
    "                logging.debug(fh)\n",
    "            elif non_sap_file.file.split('.')[-2] == 'tar':\n",
    "                fh = tarfile.open(fileobj=BytesIO(r.content), mode='r:gz')\n",
    "            elif non_sap_file.file.split('.')[-1] == 'gz':\n",
    "                fh = gzip.GzipFile(fileobj=BytesIO(r.content), mode='rb')\n",
    "            else:\n",
    "                fh = BytesIO(r.content)\n",
    "            if non_sap_file.file_type == '.csv':\n",
    "                df_up = pd.read_csv(fh, dtype='string', sep=',', quotechar='\"', encoding=encoding, chunksize=100000)\n",
    "                for i in df_up:\n",
    "                    logging.debug(i.head())\n",
    "                    buffer = BytesIO()\n",
    "                    i.to_parquet(buffer, index=False, compression='snappy')\n",
    "                    uppie.push_new_chunk(pool_id=non_sap_file.poolid, job_id=jobhandle['id'], file_path=buffer.getvalue())\n",
    "            else:\n",
    "                matches = {}\n",
    "                pd_config = {\n",
    "                            'io': fh,\n",
    "                            'sheet_name': None,\n",
    "                            'keep_default_na': False,\n",
    "                            }\n",
    "                if as_string is True:\n",
    "                    pd_config['dtype'] = str\n",
    "                df = pd.read_excel(**pd_config)\n",
    "                for a, b in product(df, df):\n",
    "                    col_a = df[a].columns\n",
    "                    col_b = df[b].columns\n",
    "                    if (len(col_a) == len(col_b)\n",
    "                        and (len([i for i, j in zip(col_a, col_b) if i == j])\n",
    "                             == len(col_b))):\n",
    "                        matches[str(col_b)] = (matches.get(str(col_b), [a, b])\n",
    "                                               + [a, b])\n",
    "                for i in matches:\n",
    "                    matches[i] = set(matches[i])\n",
    "                if (len(matches) == 1\n",
    "                    and len(df) == 1\n",
    "                    and len(copy.deepcopy(matches).popitem()[1]) == len(df)):\n",
    "                    for i in df:\n",
    "                        df = df[i]\n",
    "                elif (len(matches) == 1\n",
    "                      and len(copy.deepcopy(matches).popitem()[1]) == len(df)):\n",
    "                    dfs = []\n",
    "                    for i in df:\n",
    "                        dfs.append(df[i])\n",
    "                    df = pd.concat(dfs, ignore_index=True)\n",
    "                logging.debug(df.head())\n",
    "                buffer = BytesIO()\n",
    "                df.to_parquet(buffer, index=False, compression='snappy')\n",
    "                uppie.push_new_chunk(pool_id=non_sap_file.poolid, job_id=jobhandle['id'], file_path=buffer.getvalue())\n",
    "    except Exception as e:\n",
    "        logging.error(f'{non_sap_file} failed with error: {e}')\n",
    "        time.sleep(10)\n",
    "    uppie.submit_job(pool_id=non_sap_file.poolid, job_id=jobhandle['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibc_files_to_json(lst, name):\n",
    "    temp = []\n",
    "    for i in lst:\n",
    "        temp.append(i.to_dict())\n",
    "    with open(name, 'w') as out:\n",
    "        out.write(json.dumps(temp, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_ibc_files(jsn, url):\n",
    "    with open(jsn, 'r') as inp:\n",
    "        text = inp.read()\n",
    "    dct_lst = json.loads(text)\n",
    "    ibc_files = []\n",
    "    for entry in dct_lst:\n",
    "        entry_dct = entry\n",
    "        entry_dct['url'] = url\n",
    "        ibc_files.append(ibc_file(**entry_dct))\n",
    "    return ibc_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ibc_team():\n",
    "    def parse_url(self, url):\n",
    "        parts = []\n",
    "        connectionflag = 1\n",
    "        try:\n",
    "            parts.append(re.search('https://([a-z0-9-]+)\\.', url).groups()[0])\n",
    "            parts.append(re.search('\\.([a-z0-9-]+)\\.celonis', url).groups()[0])\n",
    "            parts.append(re.search('ui/pools/([a-z0-9-]+)', url).groups()[0])\n",
    "            try:\n",
    "                parts.append(re.search('data-connections/[a-z-]+/([a-z0-9-]+)', url)\n",
    "                             .groups()[0])\n",
    "            except AttributeError:\n",
    "                connectionflag = 0\n",
    "        except AttributeError:\n",
    "            logging.error(f'{url} this is an unvalid url.')\n",
    "        logging.debug(f'url has the following parts: {parts} and connectionflag: {connectionflag}')\n",
    "        return parts, connectionflag\n",
    "    \n",
    "    def determine_appkey(self, cmd='printenv | grep CELONIS_API_TOKEN'):\n",
    "        appkey = subprocess.run(cmd, shell=True, capture_output=True)\n",
    "        return appkey.stdout.decode('utf-8').split('=')[1].strip()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'''team: {self.team}\n",
    "                    realm: {self.realm}\n",
    "                    poolid: {self.poolid}\n",
    "                    connectionid: {self.connectionid}\n",
    "                    appkey: {self.appkey}\n",
    "                    apikey: {self.apikey}\n",
    "                    url: {self.url}'''.replace('                    ', '')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'ibc-team {self.team} in {self.realm}.'\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        parts, connectionflag = self.parse_url(url)\n",
    "        self.url = url\n",
    "        self.team = parts[0]\n",
    "        self.realm = parts[1]\n",
    "        self.poolid = parts[2]\n",
    "        if connectionflag == 1:\n",
    "            self.connectionid = parts[3]\n",
    "        else:\n",
    "            self.connectionid = None\n",
    "        self.appkey = self.determine_appkey()\n",
    "        self.apikey = self.appkey\n",
    "\n",
    "    def get_values(self):\n",
    "        return {'team': self.team,\n",
    "                'realm': self.realm,\n",
    "                'poolid': self.poolid,\n",
    "                'connectionid': self.connectionid,\n",
    "                'appkey': self.appkey,\n",
    "                'apikey': self.apikey,\n",
    "                'url': self.url,\n",
    "               }\n",
    "\n",
    "    def find_buckets(self, name=None, id=None, local_file_path=None):\n",
    "        if local_file_path is not None:\n",
    "            return [bucket(self.url, local_file_path=local_file_path )]\n",
    "        else:\n",
    "            url = f'https://{self.team}.{self.realm}.celonis.cloud/storage-manager/api/buckets?feature=SFTP'\n",
    "            header_json = {'Authorization': f'AppKey {self.appkey}', 'Accept': 'application/json'}\n",
    "            file_response = requests.get(url, headers=header_json)\n",
    "            logging.debug(str(file_response.json()))\n",
    "            if name is None and id is None:\n",
    "                return [bucket(self.url, i['id'], i['name']) for i in file_response.json()]\n",
    "            else:\n",
    "                return [bucket(self.url, i['id'], i['name']) for i in file_response.json() if i['id'] == id]\n",
    "        \n",
    "    def validate_bucket(self, name=None, id=None):\n",
    "        if name is None and id is None:\n",
    "            raise ValueError(\"either name or bucket id need to be specified.\")\n",
    "        url = f'https://{self.team}.{self.realm}.celonis.cloud/storage-manager/api/buckets?feature=SFTP'\n",
    "        header_json = {'Authorization': f'AppKey {self.appkey}', 'Accept': 'application/json'}\n",
    "        file_response = requests.get(url, headers=header_json)\n",
    "        logging.debug(str(file_response.json()))\n",
    "        result = [{'bucket_id': i['id'], 'bucket_name': i['name']} for i in file_response.json() if (i['id'] == id or i['name']==name)]\n",
    "        if len(result) == 1:\n",
    "            pass\n",
    "        elif len(result) == 0:\n",
    "            raise NameError('invalid bucket id.')\n",
    "        else:\n",
    "            raise ValueError('provided bucket identification is not unique.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bucket(ibc_team):\n",
    "    def __init__(self, url, bucket_id=None, bucket_name=None, local_file_path=None):\n",
    "        super().__init__(url)\n",
    "        if bucket_id is not None:\n",
    "            super().validate_bucket(id=bucket_id)\n",
    "        elif local_file_path is not None:\n",
    "            self.validate_folder(local_file_path)\n",
    "        else:\n",
    "            raise NameError('error initializing bucket as both bucket_id and local_file_path are None')\n",
    "        self.bucket_id = bucket_id\n",
    "        self.bucket_name = bucket_name\n",
    "        self.local_file_path = local_file_path\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{super().__str__()}\\nbucket_id: {self.bucket_id}\\nlocal_file_path: {self.local_file_path}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'bucket {self.bucket_id} in {super().__repr__()}'\n",
    "    \n",
    "    def validate_folder(self, path):\n",
    "        p = Path(path)\n",
    "        if not p.is_dir():\n",
    "            raise NameError('The folder you are refering to does not exist.')\n",
    "        glb = p.glob('*')\n",
    "        if len(list(glb)) < 1:\n",
    "            raise ValueError('The folder you specified is empty.')\n",
    "    \n",
    "    def find_folders(self, path_to_folder=''):\n",
    "        files = []\n",
    "        folders = []\n",
    "        return_folders = []\n",
    "        try:\n",
    "            if self.bucket_id is not None:\n",
    "                url = f'https://{self.team}.{self.realm}.celonis.cloud/storage-manager/api/buckets/{self.bucket_id}/files?path=/' + path_to_folder\n",
    "                header_json = {'Authorization': f'AppKey {self.appkey}', 'Accept': 'application/json'}\n",
    "                file_response = requests.get(url, headers=header_json)\n",
    "                logging.debug(file_response.json()['children'])\n",
    "                for i in file_response.json()['children']:\n",
    "                    if i['type'] == 'FILE':\n",
    "                        files.append({'size': i['size'], 'file': (path_to_folder + i['filename'])})\n",
    "                    elif i['type'] == 'DIRECTORY':\n",
    "                        folders.append(path_to_folder + i['filename'] + '/')\n",
    "            else:\n",
    "                # TODO: find folders locally\n",
    "                glb = Path(path_to_folder).glob('*')\n",
    "                for glb_object in glb:\n",
    "                    if glb_object.is_file():\n",
    "                        files.append({'size': '', 'file':glb_object})\n",
    "                    elif glb_object.is_dir():\n",
    "                        folders.append(glb_object)\n",
    "            logging.debug(f'{files}\\n\\n{folders}\\n')\n",
    "            if len(folders) > 0:\n",
    "                for f in folders:\n",
    "                    return_folders.extend(self.find_folders(f))\n",
    "            if len(files) > 0:\n",
    "                return_folders.append(folder(url=self.url, bucket_id=self.bucket_id, folder=path_to_folder, files=files, local_file_path=path_to_folder))\n",
    "            return return_folders\n",
    "        except Exception as e:\n",
    "            logging.error(f'{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class folder(bucket):\n",
    "    def __init__(self, url, folder, files, bucket_id=None, local_file_path=None):\n",
    "        if bucket_id is not None:\n",
    "            super().__init__(url, bucket_id=bucket_id)\n",
    "        elif local_file_path is not None:\n",
    "            super().__init__(url, local_file_path=local_file_path)\n",
    "        else:\n",
    "            raise NameError('error initializing folder as both bucket_id and local_file_path are None')\n",
    "        self.folder = folder\n",
    "        self.files = files\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{super().__str__()}\\nfolder: {self.folder}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.folder} in {self.bucket_id}'\n",
    "    \n",
    "    def classify_files(self):\n",
    "        logging.debug(f'{self.files[0]} is the first file from {len(self.files)} to be classified.')\n",
    "        head, body = [], []\n",
    "        for file in self.files:\n",
    "            file_characteristics = {\n",
    "                'url': self.url,\n",
    "                'bucket_id': self.bucket_id,\n",
    "                'local_file_path': self.local_file_path,\n",
    "                'folder': self.folder,\n",
    "                'file': file['file'],\n",
    "                'size': file['size'],\n",
    "                'file_type': None,\n",
    "                'header': False,\n",
    "                'encryption': None,\n",
    "                'compression': None,\n",
    "            }\n",
    "            p = Path(file['file'])\n",
    "            logging.debug(f'file {p} has suffixes {p.suffixes}')\n",
    "            \n",
    "            if len(re.findall(sap_file_type, p.name)):\n",
    "                file_characteristics['file_type'] = 'sap'\n",
    "                if '_HEADER_' in p.name:\n",
    "                    file_characteristics['header'] = True\n",
    "            else:\n",
    "                for s in p.suffixes:\n",
    "                    if s.lower() in generic_file_type:\n",
    "                        file_characteristics['file_type'] = s.lower()\n",
    "                    elif s.lower() in compressed:\n",
    "                        if file_characteristics['compression'] is None:\n",
    "                            file_characteristics['compression'] = s.lower()\n",
    "                        else:\n",
    "                            file_characteristics['compression'] += s.lower()\n",
    "                    elif s.lower() in encrypted:\n",
    "                        if file_characteristics['encryption'] is None:\n",
    "                            file_characteristics['encryption'] = s.lower()\n",
    "                        else:\n",
    "                            file_characteristics['encryption'] += s.lower()\n",
    "            logging.debug(f'file {file[\"file\"]} has the following traits: {file_characteristics}')\n",
    "            if file_characteristics['file_type'] is None and file_characteristics['compression'] is None:\n",
    "                logging.warning(f'{file[\"file\"]} with traits: {file_characteristics} is of wrong file type.')\n",
    "            elif file_characteristics['file_type'] != 'sap' or file_characteristics['header'] is True:\n",
    "                file_tmp = ibc_file(**file_characteristics)\n",
    "                logging.debug(file_tmp)\n",
    "                head.append(file_tmp)\n",
    "            else:\n",
    "                file_tmp = ibc_file(**file_characteristics)\n",
    "                logging.debug(file_tmp)\n",
    "                body.append(file_tmp)\n",
    "        return head, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ibc_file(folder):\n",
    "    # ABAP, Header, csv, gz, zip, 7z, pgp, gpg\n",
    "    def __init__(self, url, folder, file, file_type, encryption, header, compression, size=None, files=None, bucket_id=None, local_file_path=None):\n",
    "        super().__init__(url=url, bucket_id=bucket_id, folder=folder, files=files, local_file_path=local_file_path)\n",
    "        self.file = file\n",
    "        self.file_type = file_type\n",
    "        self.encryption = encryption\n",
    "        self.header = header\n",
    "        self.compression = compression\n",
    "        self.bucket_id = bucket_id\n",
    "        self.folder = folder\n",
    "        self.size = size\n",
    "        self.local_file_path = local_file_path\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{super().__str__()}\\nfile: {self.file}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.file} of {self.file_type} with header being {self.header}'\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {'file': str(self.file),\n",
    "                'file_type': self.file_type,\n",
    "                'encryption': self.encryption,\n",
    "                'header': self.header,\n",
    "                'compression': self.compression,\n",
    "                'bucket_id': self.bucket_id,\n",
    "                'folder': str(self.folder),\n",
    "                'size': self.size,\n",
    "                'local_file_path': str(self.local_file_path),\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ibc_team(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exclude_loaded is True:\n",
    "    data = [] #determine_tables_loaded(c)\n",
    "else:\n",
    "    data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if continue_from_last_time is True and Path('./head.json').is_file():\n",
    "    logging.info('getting ibc_files from ML Workbench')\n",
    "    if Path('./body.json').is_file():\n",
    "        body = json_to_ibc_files('body.json', url)\n",
    "    else:\n",
    "        body = []\n",
    "    head = json_to_ibc_files('head.json', url)\n",
    "else:\n",
    "    logging.info('getting ibc_files from SFTP')\n",
    "    head, body = [], []\n",
    "    buckets = c.find_buckets(local_file_path=local_file_path)\n",
    "    for b in buckets:\n",
    "        logging.info(f'started finding folders at: {datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "        f = b.find_folders(path_to_folder)\n",
    "        logging.info(f'finished finding folders at: {datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "        try:\n",
    "            logging.info(f'started classifying files at: {datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "            for i in f:\n",
    "                head_instance, body_instance = i.classify_files()\n",
    "                head.extend(head_instance)\n",
    "                body.extend(body_instance)\n",
    "            logging.info(f'finished classifying files at: {datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "        except Exception as e:\n",
    "            logging.error(f'encounterd error {e} while processing {i} in {f}')\n",
    "        ibc_files_to_json(head, 'head.json')\n",
    "        ibc_files_to_json(body, 'body.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lenght_before = len(head)\n",
    "head = list(head)\n",
    "for date in data:\n",
    "    for h in head:\n",
    "        if date == Path(h.file).name.split('_HEADER_')[0]:\n",
    "            print(date, Path(h.file).name.split('_HEADER_')[0])\n",
    "            head.remove(h)\n",
    "head = tuple(head)\n",
    "body = tuple(body)\n",
    "\"\"\"\n",
    "logging.info(f'finished classifying files. {len(head)} header and {len(body)} sub files were found.') # {lenght_before - len(head)} files were skipped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if look_for_sap_files_globally is True:\n",
    "    location_indicator = 'global'\n",
    "else:\n",
    "    location_indicator = 'local'\n",
    "jobstatus = {}\n",
    "uppie = cloud(tenant=c.team, realm=c.realm, api_key=c.apikey)\n",
    "for header in head:\n",
    "    if header.file_type == 'sap':\n",
    "        import_sap_header(header, body, jobstatus, uppie, data, delta=False, location_indicator=location_indicator)\n",
    "    else:\n",
    "        continue\n",
    "        import_non_sap_file(header, jobstatus, uppie, data, delta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('upload done.')\n",
    "running = True\n",
    "while running:\n",
    "    jobs = uppie.list_jobs(c.poolid)\n",
    "    for jobids in jobstatus:\n",
    "        for i in jobs:\n",
    "            try:\n",
    "                if i['id'] == jobids:\n",
    "                    if i['status'] == 'QUEUED':\n",
    "                        pass\n",
    "                    elif jobstatus[jobids] is True:\n",
    "                        pass\n",
    "                    elif i['status'] == 'DONE':\n",
    "                        jobstatus[jobids] = True\n",
    "                    elif i['status'] != 'RUNNING':\n",
    "                        jobstatus[jobids] = True\n",
    "                    else:\n",
    "                        pass\n",
    "                    break\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                logging.error('terminating program\\n')\n",
    "                quit()\n",
    "            except:\n",
    "                pass\n",
    "    if all(status is True for status in jobstatus.values()):\n",
    "        running = False\n",
    "        for i in jobs:\n",
    "            if i['id'] in jobstatus:\n",
    "                if i['status'] == 'DONE':\n",
    "                    logging.info(f\"{i['targetName']} was successfully installed in the database\")\n",
    "                else:\n",
    "                    logging.error(f\"{i['targetName']} failed with: {i}\")\n",
    "    else:\n",
    "        time.sleep(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
